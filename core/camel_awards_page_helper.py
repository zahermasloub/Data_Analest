# -*- coding: utf-8 -*-
"""
Ù…Ø³Ø§Ø¹Ø¯ ØµÙØ­Ø© ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ§Ø¦Ø² Ø³Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù‡Ø¬Ù†
Camel Awards Page Helper for Streamlit Integration
===================================================
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, Tuple, Optional

from core.enhanced_audit_system import (
    DataNormalizer,
    EnhancedBankMatcher,
    GroundTruthValidator,
    ComprehensiveReportGenerator
)


def detect_duplicates_enhanced(
    df: pd.DataFrame,
    normalizer: DataNormalizer
) -> pd.DataFrame:
    """
    ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨
    
    Composite Key:
        Season + Race + OwnerNumber + OwnerName + OwnerQatariID + AwardAmount
    """
    # Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    required_fields = ['Season', 'Race', 'OwnerNumber', 'OwnerName', 'OwnerQatariID', 'AwardAmount']
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø­Ù‚ÙˆÙ„
    missing_fields = [f for f in required_fields if f not in df.columns]
    if missing_fields:
        print(f"âš ï¸ Ø­Ù‚ÙˆÙ„ Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_fields}")
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·
        available_fields = [f for f in required_fields if f in df.columns]
        if not available_fields:
            return pd.DataFrame()
    else:
        available_fields = required_fields
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨ (Ø§Ø³ØªØ®Ø¯Ø§Ù… list comprehension Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† += Ù„ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ NotImplementedType)
    df_composite = df.copy()
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©
    key_parts = []
    for field in available_fields:
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¢Ù…Ù† Ù„Ù€ string Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            field_values = df_composite[field].fillna('').apply(lambda x: str(x) if pd.notna(x) else '')
            key_parts.append(field_values)
        except Exception as e:
            print(f"   âš ï¸ ØªØ®Ø·ÙŠ Ø­Ù‚Ù„ {field} ÙÙŠ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨: {str(e)}")
            continue
    
    # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø¨Ù€ '|'
    if key_parts:
        df_composite['CompositeKey'] = key_parts[0].astype(str)
        for part in key_parts[1:]:
            df_composite['CompositeKey'] = df_composite['CompositeKey'] + '|' + part.astype(str)
    else:
        # ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø­Ù‚ÙˆÙ„ ØµØ§Ù„Ø­Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… index
        df_composite['CompositeKey'] = df_composite.index.astype(str)
    
    df_composite['CompositeKey'] = df_composite['CompositeKey'].str.strip('|')
    
    # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    key_counts = df_composite['CompositeKey'].value_counts()
    duplicates = key_counts[key_counts > 1]
    
    if len(duplicates) == 0:
        return pd.DataFrame()
    
    # ØªØµÙÙŠØ© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
    duplicates_df = df_composite[df_composite['CompositeKey'].isin(duplicates.index)].copy()
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙƒØ±Ø§Ø±
    duplicates_df['DuplicateCount'] = duplicates_df['CompositeKey'].map(key_counts)
    duplicates_df['DuplicateGroup'] = duplicates_df.groupby('CompositeKey').ngroup() + 1
    
    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
    duplicates_df = duplicates_df.sort_values(['DuplicateGroup', 'CompositeKey'])
    
    return duplicates_df


def run_comprehensive_audit(
    awards_data: pd.DataFrame,
    bank_data: Optional[pd.DataFrame] = None,
    ground_truth_data: Optional[pd.DataFrame] = None,
    use_composite_key: bool = True,
    enable_bank_matching: bool = True,
    enable_ground_truth: bool = True
) -> Dict:
    """
    ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø´Ø§Ù…Ù„
    
    Args:
        awards_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬ÙˆØ§Ø¦Ø²
        bank_data: ÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        ground_truth_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø£Ø±Ø¶ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        use_composite_key: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨
        enable_bank_matching: ØªÙØ¹ÙŠÙ„ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¨Ù†Ùƒ
        enable_ground_truth: ØªÙØ¹ÙŠÙ„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø£Ø±Ø¶ÙŠØ©
        
    Returns:
        Dict ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    """
    results = {}
    
    # 1. ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print("ğŸ“Š ØªØ·Ø¨ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬ÙˆØ§Ø¦Ø²...")
    normalizer = DataNormalizer()
    normalized_awards = normalizer.normalize_dataframe(
        awards_data.copy(),
        df_name="Awards"
    )
    results['normalized_awards'] = normalized_awards
    results['normalization_mapping'] = normalizer.get_mapping_documentation()
    
    # 2. ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    print("ğŸ” ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª...")
    if use_composite_key:
        duplicates = detect_duplicates_enhanced(normalized_awards, normalizer)
    else:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø³ÙŠØ·Ø©
        duplicates = normalized_awards[normalized_awards.duplicated(keep=False)]
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙÙŠ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª (PyArrow Ù„Ø§ ÙŠØ¯Ø¹Ù…Ù‡Ø§)
    if not duplicates.empty and duplicates.columns.duplicated().any():
        print(f"   âš ï¸ Ø¥Ø²Ø§Ù„Ø© {duplicates.columns.duplicated().sum()} Ø¹Ù…ÙˆØ¯ Ù…ÙƒØ±Ø± Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª")
        duplicates = duplicates.loc[:, ~duplicates.columns.duplicated(keep='first')]
    
    results['duplicates'] = duplicates
    results['duplicate_stats'] = {
        'total_records': len(normalized_awards),
        'duplicate_records': len(duplicates),
        'duplicate_percentage': (len(duplicates) / len(normalized_awards) * 100) if len(normalized_awards) > 0 else 0,
        'unique_duplicate_groups': duplicates['DuplicateGroup'].nunique() if 'DuplicateGroup' in duplicates.columns else 0
    }
    
    # 3. Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¨Ù†Ùƒ
    bank_matches = None
    bank_match_stats = None
    
    if enable_bank_matching and bank_data is not None and len(bank_data) > 0:
        print("ğŸ¦ Ù…Ø·Ø§Ø¨Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ...")
        try:
            matcher = EnhancedBankMatcher()
            bank_matches = matcher.match_awards_to_bank(
                normalized_awards,
                bank_data.copy()
            )
            
            # Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
            if bank_matches is not None and len(bank_matches) > 0:
                bank_match_stats = {
                    'total_matched': len(bank_matches[bank_matches['MatchStatus'].str.contains('Matched', case=False, na=False)]),
                    'exact_matches': len(bank_matches[bank_matches['MatchStatus'] == 'Matched']),
                    'fuzzy_matches': len(bank_matches[bank_matches['MatchStatus'] == 'Partial Match']),
                    'not_matched': len(bank_matches[bank_matches['MatchStatus'] == 'Not Matched'])
                }
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¨Ù†Ùƒ: {e}")
            bank_matches = None
            bank_match_stats = None
    
    results['bank_matches'] = bank_matches
    results['bank_match_stats'] = bank_match_stats
    
    # 4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø£Ø±Ø¶ÙŠØ©
    validation_results = None
    validation_metrics = None
    
    if enable_ground_truth and ground_truth_data is not None and len(ground_truth_data) > 0:
        print("âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„Ø£Ø±Ø¶ÙŠØ©...")
        try:
            validator = GroundTruthValidator()
            validation_results = validator.validate_detection(duplicates)
            
            if validation_results:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
                validation_metrics = {
                    'accuracy': validation_results.get('accuracy', 0),
                    'recall': validation_results.get('recall', 0),
                    'precision': validation_results.get('precision', 0),
                    'f1_score': validation_results.get('f1_score', 0),
                    'true_positive': validation_results.get('true_positive', 0),
                    'false_positive': validation_results.get('false_positive', 0),
                    'true_negative': validation_results.get('true_negative', 0),
                    'false_negative': validation_results.get('false_negative', 0),
                    'confusion_matrix': {
                        'true_positive': validation_results.get('true_positive', 0),
                        'false_positive': validation_results.get('false_positive', 0),
                        'true_negative': validation_results.get('true_negative', 0),
                        'false_negative': validation_results.get('false_negative', 0)
                    }
                }
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚: {e}")
            validation_results = None
            validation_metrics = None
    
    results['validation'] = validation_results
    results['validation_metrics'] = validation_metrics
    
    return results


def generate_reports(
    results: Dict,
    output_dir: Path,
    timestamp: str
) -> Dict[str, str]:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„Ø©
    
    Args:
        results: Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚
        output_dir: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸
        timestamp: Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ
        
    Returns:
        Dict ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù…Ù† results
    # (Ù„Ø£Ù† ComprehensiveReportGenerator ÙŠØ­ØªØ§Ø¬ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙˆÙ„ÙŠØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø©)
    
    # Ø¥Ù†Ø´Ø§Ø¡ normalizer Ø¬Ø¯ÙŠØ¯
    normalizer = DataNormalizer()
    
    # Ø¥Ù†Ø´Ø§Ø¡ bank_matcher
    bank_matcher = EnhancedBankMatcher()
    
    # Ø¥Ù†Ø´Ø§Ø¡ ground_truth_validator
    ground_truth_validator = GroundTruthValidator()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø¨Ø§Ù„ØªÙˆÙ‚ÙŠØ¹ Ø§Ù„ØµØ­ÙŠØ­
    report_generator = ComprehensiveReportGenerator(
        duplicates=results['duplicates'],
        normalizer=normalizer,
        bank_matcher=bank_matcher,
        ground_truth_validator=ground_truth_validator,
        output_dir=str(output_dir)
    )
    
    report_paths = report_generator.generate_all_reports()
    
    return report_paths
