"""
Strict Audit Analyzer - 100% Accuracy Version
==============================================

Ù†Ø¸Ø§Ù… ØªØ¯Ù‚ÙŠÙ‚ ØµØ§Ø±Ù… Ø¨Ø¯Ù‚Ø© 100% Ù„Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ©

Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:
1. ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø¯Ù‚Ø© 100%:
   - Ù†ÙØ³ Season
   - Ù†ÙØ³ Race
   - Ù†ÙØ³ Owner Number
   - Ù†ÙØ³ Owner Name
   - Ù†ÙØ³ Owner QatariId
   - Ù†ÙØ³ Award Amount
   
2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨Ù†Ùƒ Ø¨Ø¯Ù‚Ø© 100%:
   - Ù…Ø·Ø§Ø¨Ù‚Ø© Reference Number Ø¯Ù‚ÙŠÙ‚Ø©
   - Ø§Ù„Ù…Ø¨Ù„Øº Ù…ØªØ·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹ (Ø¨Ø¯ÙˆÙ† ØªØ³Ø§Ù…Ø­)
   - Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù…Ù„Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Any
import re


class StrictAuditAnalyzer:
    """
    Ù…Ø­Ù„Ù„ ØªØ¯Ù‚ÙŠÙ‚ ØµØ§Ø±Ù… Ø¨Ù…Ø¹Ø§ÙŠÙŠØ± 100%
    
    Features:
    - Zero tolerance for data inconsistencies
    - Exact composite key matching
    - Strict reference validation
    - Complete data validation
    - Detailed audit trail
    """
    
    def __init__(self):
        self.awards_data = None
        self.bank_data = None
        self.duplicates = None
        self.validation_report = {
            'warnings': [],
            'errors': [],
            'statistics': {}
        }
        
        # Strict parameters - NO tolerance
        self.AMOUNT_TOLERANCE = 0.00  # Must be EXACT
        self.REQUIRE_ALL_FIELDS = True  # All fields mandatory
        
    def normalize_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ ØµØ§Ø±Ù…"""
        if pd.isna(text) or text is None:
            return None
        
        text = str(text).strip()
        text = ' '.join(text.split())  # Remove multiple spaces
        text = text.lower()
        
        return text if text else None
    
    def validate_awards_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØµØ§Ø±Ù… Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬ÙˆØ§Ø¦Ø²
        
        Returns:
            (cleaned_df, list_of_errors)
        """
        errors = []
        df = df.copy()
        
        # Required fields for duplicate detection
        required_fields = [
            'Season', 'Race', 'OwnerNumber', 
            'OwnerName', 'OwnerQatariId', 'AwardAmount'
        ]
        
        # Check all required fields exist
        missing_fields = [f for f in required_fields if f not in df.columns]
        if missing_fields:
            errors.append(f"âŒ CRITICAL: Missing required fields: {missing_fields}")
            return df, errors
        
        # Count initial records
        initial_count = len(df)
        
        # Remove rows with ANY missing key field
        for field in required_fields:
            before = len(df)
            df = df[df[field].notna()].copy()
            removed = before - len(df)
            if removed > 0:
                errors.append(f"âš ï¸ Removed {removed} rows with missing {field}")
        
        # Validate and clean each field
        
        # 1. Season (must be non-empty string)
        df['Season'] = df['Season'].astype(str).str.strip()
        invalid_season = df[df['Season'].isin(['', 'nan', 'None'])]
        if len(invalid_season) > 0:
            errors.append(f"âš ï¸ {len(invalid_season)} rows with invalid Season")
            df = df[~df.index.isin(invalid_season.index)]
        
        # 2. Race (must be non-empty string)
        df['Race'] = df['Race'].astype(str).str.strip()
        invalid_race = df[df['Race'].isin(['', 'nan', 'None'])]
        if len(invalid_race) > 0:
            errors.append(f"âš ï¸ {len(invalid_race)} rows with invalid Race")
            df = df[~df.index.isin(invalid_race.index)]
        
        # 3. Owner Number (must be valid)
        df['OwnerNumber'] = df['OwnerNumber'].astype(str).str.strip()
        invalid_owner_num = df[df['OwnerNumber'].isin(['', 'nan', 'None'])]
        if len(invalid_owner_num) > 0:
            errors.append(f"âš ï¸ {len(invalid_owner_num)} rows with invalid OwnerNumber")
            df = df[~df.index.isin(invalid_owner_num.index)]
        
        # 4. Owner Name (must be non-empty)
        df['OwnerName'] = df['OwnerName'].astype(str).str.strip()
        invalid_name = df[df['OwnerName'].isin(['', 'nan', 'None'])]
        if len(invalid_name) > 0:
            errors.append(f"âš ï¸ {len(invalid_name)} rows with invalid OwnerName")
            df = df[~df.index.isin(invalid_name.index)]
        
        # 5. Owner QatariID (must be valid)
        df['OwnerQatariId'] = df['OwnerQatariId'].astype(str).str.strip()
        invalid_id = df[df['OwnerQatariId'].isin(['', 'nan', 'None'])]
        if len(invalid_id) > 0:
            errors.append(f"âš ï¸ {len(invalid_id)} rows with invalid OwnerQatariId")
            df = df[~df.index.isin(invalid_id.index)]
        
        # 6. Award Amount (must be positive number)
        df['AwardAmount'] = pd.to_numeric(df['AwardAmount'], errors='coerce')
        invalid_amount = df[(df['AwardAmount'].isna()) | (df['AwardAmount'] <= 0)]
        if len(invalid_amount) > 0:
            errors.append(f"âš ï¸ {len(invalid_amount)} rows with invalid AwardAmount")
            df = df[~df.index.isin(invalid_amount.index)]
        
        # Summary
        final_count = len(df)
        removed_total = initial_count - final_count
        if removed_total > 0:
            errors.append(f"ğŸ“Š VALIDATION SUMMARY: Removed {removed_total} invalid rows ({removed_total/initial_count*100:.2f}%)")
            errors.append(f"âœ… Valid records: {final_count:,} ({final_count/initial_count*100:.2f}%)")
        
        return df, errors
    
    def detect_strict_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø¯Ù‚Ø© 100%
        
        Composite Key (ALL must match EXACTLY):
        - Season
        - Race
        - OwnerNumber
        - OwnerName
        - OwnerQatariID
        - AwardAmount
        
        EntryDate is ALLOWED to differ (this is how we detect duplicates)
        """
        print("\n" + "="*80)
        print("ğŸ” ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ù…Ø¹ÙŠØ§Ø± 100% Ø¯Ù‚Ø©")
        print("="*80)
        
        # Validate data first
        df, validation_errors = self.validate_awards_data(df)
        
        if validation_errors:
            print("\nâš ï¸ ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
            for error in validation_errors:
                print(f"   {error}")
        
        # Define composite key fields
        key_fields = ['Season', 'Race', 'OwnerNumber', 'OwnerName', 'OwnerQatariId', 'AwardAmount']
        
        print(f"\nğŸ“‹ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨ (Composite Key):")
        for i, field in enumerate(key_fields, 1):
            print(f"   {i}. {field}")
        
        # Normalize text fields for matching
        df_normalized = df.copy()
        for field in ['Season', 'Race', 'OwnerName']:
            if field in df_normalized.columns:
                df_normalized[f'{field}_normalized'] = df_normalized[field].apply(self.normalize_text)
        
        # Create composite key (using normalized values)
        df_normalized['_CompositeKey'] = (
            df_normalized['Season'].astype(str).str.strip().str.lower() + '|' +
            df_normalized['Race'].astype(str).str.strip().str.lower() + '|' +
            df_normalized['OwnerNumber'].astype(str).str.strip() + '|' +
            df_normalized['OwnerName'].astype(str).str.strip().str.lower() + '|' +
            df_normalized['OwnerQatariId'].astype(str).str.strip() + '|' +
            df_normalized['AwardAmount'].round(2).astype(str)
        )
        
        # Count occurrences
        df_normalized['_DuplicateCount'] = df_normalized.groupby('_CompositeKey')['_CompositeKey'].transform('count')
        df_normalized['_DuplicateGroup'] = df_normalized.groupby('_CompositeKey').ngroup()
        
        # Filter only duplicates (count >= 2)
        duplicates = df_normalized[df_normalized['_DuplicateCount'] >= 2].copy()
        
        # Sort by group and entry date
        if 'EntryDate' in duplicates.columns:
            duplicates['EntryDate'] = pd.to_datetime(duplicates['EntryDate'], errors='coerce')
            duplicates = duplicates.sort_values(['_DuplicateGroup', 'EntryDate'])
        else:
            duplicates = duplicates.sort_values('_DuplicateGroup')
        
        # Add duplicate severity classification
        duplicates['_DuplicateSeverity'] = duplicates.apply(self._classify_duplicate_severity, axis=1)
        
        # Statistics
        total_records = len(df)
        total_duplicates = len(duplicates)
        unique_groups = duplicates['_DuplicateGroup'].nunique() if len(duplicates) > 0 else 0
        total_amount = duplicates['AwardAmount'].sum() if len(duplicates) > 0 else 0
        duplicate_rate = (total_duplicates / total_records * 100) if total_records > 0 else 0
        
        print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙƒØ´Ù:")
        print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª: {total_records:,}")
        print(f"   Ø³Ø¬Ù„Ø§Øª Ù…ÙƒØ±Ø±Ø©: {total_duplicates:,}")
        print(f"   Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØªÙƒØ±Ø§Ø±: {unique_groups:,}")
        print(f"   Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±: {duplicate_rate:.2f}%")
        print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¨Ø§Ù„Øº Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {total_amount:,.2f} Ø±ÙŠØ§Ù„")
        
        if unique_groups > 0:
            print(f"\nğŸ” Ø£ÙƒØ«Ø± Ø§Ù„Ø­Ø§Ù„Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹:")
            top_duplicates = duplicates.groupby('_CompositeKey').size().sort_values(ascending=False).head(5)
            for i, (key, count) in enumerate(top_duplicates.items(), 1):
                example = duplicates[duplicates['_CompositeKey'] == key].iloc[0]
                print(f"   {i}. {example['OwnerName']} - {example['Race']} ({count} Ù…Ø±Ø§Øª)")
        
        self.duplicates = duplicates
        self.validation_report['statistics']['duplicates'] = {
            'total_records': total_records,
            'total_duplicates': total_duplicates,
            'unique_groups': unique_groups,
            'duplicate_rate': duplicate_rate,
            'total_amount': float(total_amount)
        }
        
        return duplicates
    
    def _classify_duplicate_severity(self, row) -> str:
        """ØªØµÙ†ÙŠÙ Ø®Ø·ÙˆØ±Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        # Check if same entry date (more suspicious)
        # This would require comparing within group, for now return default
        return "Ù…Ø´ØªØ¨Ù‡"  # Can be enhanced with more logic
    
    def verify_bank_strict(self, duplicates: pd.DataFrame, bank_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØµØ§Ø±Ù… Ù…Ù† ÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ
        
        Matching criteria:
        1. Reference Number must match EXACTLY (last 10 digits)
        2. Amount must match EXACTLY (no tolerance)
        3. All bank data must be complete
        
        Returns:
            Dictionary with 'matched', 'unmatched' DataFrames
        """
        print("\n" + "="*80)
        print("ğŸ¦ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ Ø¨Ù…Ø¹ÙŠØ§Ø± 100% Ø¯Ù‚Ø©")
        print("="*80)
        
        matched_records = []
        unmatched_records = []
        match_details = []
        
        # Prepare bank data
        bank_df = bank_df.copy()
        
        # Ensure reference fields are strings and cleaned
        for ref_field in ['AwardRef', 'AwardRef10Digits', 'BankReference']:
            if ref_field in bank_df.columns:
                bank_df[ref_field] = bank_df[ref_field].astype(str).str.strip().str.replace(r'[^\w]', '', regex=True).str.lower()
        
        # Ensure amount is numeric
        if 'TransferAmount' in bank_df.columns:
            bank_df['TransferAmount'] = pd.to_numeric(bank_df['TransferAmount'], errors='coerce')
        
        print(f"\nğŸ“‹ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©:")
        print(f"   1. Reference Number: ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚ (Ø¢Ø®Ø± 10 Ø£Ø±Ù‚Ø§Ù…)")
        print(f"   2. Award Amount: ØªØ·Ø§Ø¨Ù‚ 100% (Ø¨Ø¯ÙˆÙ† ØªØ³Ø§Ù…Ø­)")
        print(f"   3. Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: ÙƒØ§Ù…Ù„Ø© ÙˆØµØ­ÙŠØ­Ø©")
        
        print(f"\nğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(duplicates):,} Ø³Ø¬Ù„...")
        
        for idx, award_row in duplicates.iterrows():
            # Extract reference numbers
            award_refs = []
            
            # Check PaymentReference
            if 'PaymentReference' in award_row and pd.notna(award_row['PaymentReference']):
                ref = str(award_row['PaymentReference']).strip()
                if ref and ref.lower() not in ['nan', 'none', '']:
                    award_refs.append(ref)
            
            # Check PaymentReference_D1
            if 'PaymentReference_D1' in award_row and pd.notna(award_row['PaymentReference_D1']):
                ref = str(award_row['PaymentReference_D1']).strip()
                if ref and ref.lower() not in ['nan', 'none', '']:
                    award_refs.append(ref)
            
            # Get award amount
            award_amount = award_row.get('AwardAmount', 0)
            
            # If no references, mark as unmatched
            if not award_refs:
                award_dict = award_row.to_dict()
                award_dict['MatchStatus'] = 'NO_REFERENCE'
                award_dict['MatchReason'] = 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ù‚Ù… Ù…Ø±Ø¬Ø¹ÙŠ ÙÙŠ Ø³Ø¬Ù„ Ø§Ù„Ø¬Ø§Ø¦Ø²Ø©'
                unmatched_records.append(award_dict)
                continue
            
            # Try to find EXACT match in bank
            match_found = False
            best_match = None
            
            for award_ref in award_refs:
                # Clean reference
                award_ref_clean = re.sub(r'[^\w]', '', award_ref).lower()
                
                if len(award_ref_clean) < 10:
                    continue
                
                # Get last 10 digits
                award_ref_last10 = award_ref_clean[-10:]
                
                # Find matches by reference
                ref_matches = bank_df[
                    (bank_df['AwardRef'].str.contains(award_ref_last10, na=False, case=False)) |
                    (bank_df['AwardRef10Digits'].str.contains(award_ref_last10, na=False, case=False))
                ]
                
                if len(ref_matches) > 0:
                    # Check amount EXACTLY
                    exact_matches = ref_matches[
                        (ref_matches['TransferAmount'].notna()) &
                        (abs(ref_matches['TransferAmount'] - award_amount) <= self.AMOUNT_TOLERANCE)
                    ]
                    
                    if len(exact_matches) > 0:
                        best_match = exact_matches.iloc[0]
                        match_found = True
                        break
            
            # Categorize result
            award_dict = award_row.to_dict()
            
            if match_found and best_match is not None:
                # MATCHED - 100% criteria met
                award_dict['MatchStatus'] = 'MATCHED_100'
                award_dict['MatchReason'] = f'Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¯Ù‚ÙŠÙ‚Ø© 100%: Reference + Amount'
                award_dict['BankTransferAmount'] = best_match['TransferAmount']
                award_dict['BankTransactionDate'] = best_match.get('TransactionDate')
                award_dict['BankValueDate'] = best_match.get('ValueDate')
                award_dict['BankBeneficiary'] = best_match.get('BeneficiaryName')
                award_dict['BankReference'] = best_match.get('BankReference')
                award_dict['BankIBAN'] = best_match.get('IBAN')
                award_dict['AmountDifference'] = 0.00
                
                matched_records.append(award_dict)
                
                match_details.append({
                    'OwnerName': award_row.get('OwnerName'),
                    'AwardAmount': award_amount,
                    'BankAmount': best_match['TransferAmount'],
                    'Reference': award_refs[0] if award_refs else 'N/A',
                    'Status': 'âœ… Ù…Ø·Ø§Ø¨Ù‚ 100%'
                })
            else:
                # UNMATCHED
                award_dict['MatchStatus'] = 'UNMATCHED'
                
                # Determine specific reason
                if not award_refs:
                    award_dict['MatchReason'] = 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ù‚Ù… Ù…Ø±Ø¬Ø¹ÙŠ'
                else:
                    # Check if reference exists but amount differs
                    ref_check = False
                    for award_ref in award_refs:
                        award_ref_clean = re.sub(r'[^\w]', '', award_ref).lower()
                        if len(award_ref_clean) >= 10:
                            award_ref_last10 = award_ref_clean[-10:]
                            ref_exists = bank_df[
                                (bank_df['AwardRef'].str.contains(award_ref_last10, na=False, case=False)) |
                                (bank_df['AwardRef10Digits'].str.contains(award_ref_last10, na=False, case=False))
                            ]
                            if len(ref_exists) > 0:
                                ref_check = True
                                break
                    
                    if ref_check:
                        award_dict['MatchReason'] = 'Reference Ù…ÙˆØ¬ÙˆØ¯ Ù„ÙƒÙ† Ø§Ù„Ù…Ø¨Ù„Øº ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚'
                    else:
                        award_dict['MatchReason'] = 'Reference ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ'
                
                unmatched_records.append(award_dict)
                
                match_details.append({
                    'OwnerName': award_row.get('OwnerName'),
                    'AwardAmount': award_amount,
                    'Reference': award_refs[0] if award_refs else 'N/A',
                    'Status': 'âŒ ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚'
                })
        
        # Convert to DataFrames
        matched_df = pd.DataFrame(matched_records) if matched_records else pd.DataFrame()
        unmatched_df = pd.DataFrame(unmatched_records) if unmatched_records else pd.DataFrame()
        
        # Statistics
        total = len(duplicates)
        matched_count = len(matched_df)
        unmatched_count = len(unmatched_df)
        match_rate = (matched_count / total * 100) if total > 0 else 0
        
        print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚:")
        print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª: {total:,}")
        print(f"   âœ… Ù…Ø·Ø§Ø¨Ù‚ 100%: {matched_count:,} ({match_rate:.1f}%)")
        print(f"   âŒ ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚: {unmatched_count:,} ({100-match_rate:.1f}%)")
        
        if unmatched_count > 0:
            print(f"\nâš ï¸ Ø£Ø³Ø¨Ø§Ø¨ Ø¹Ø¯Ù… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©:")
            if len(unmatched_df) > 0 and 'MatchReason' in unmatched_df.columns:
                reason_counts = unmatched_df['MatchReason'].value_counts()
                for reason, count in reason_counts.items():
                    print(f"   â€¢ {reason}: {count} Ø³Ø¬Ù„")
        
        self.validation_report['statistics']['bank_verification'] = {
            'total': total,
            'matched': matched_count,
            'unmatched': unmatched_count,
            'match_rate': match_rate
        }
        
        return {
            'matched': matched_df,
            'unmatched': unmatched_df,
            'details': pd.DataFrame(match_details)
        }
    
    def generate_strict_reports(self, output_dir: str = "outputs") -> Dict[str, str]:
        """
        Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø§Ø±ÙŠØ± ØµØ§Ø±Ù…Ø© Ø¨Ø¯Ù‚Ø© 100%
        
        Reports:
        1. Strict_Duplicates_Report.xlsx
        2. Strict_Bank_Verification.xlsx
        3. Validation_Report.xlsx
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        generated_files = {}
        
        print("\n" + "="*80)
        print("ğŸ“„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„ØµØ§Ø±Ù…Ø©")
        print("="*80)
        
        # Report 1: Strict Duplicates
        if self.duplicates is not None and len(self.duplicates) > 0:
            duplicates_file = output_path / f"Strict_Duplicates_{timestamp}.xlsx"
            
            print(f"\nğŸ“ Ø§Ù„ØªÙ‚Ø±ÙŠØ± 1: ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„ØµØ§Ø±Ù…")
            print(f"   Ø§Ù„Ù…Ù„Ù: {duplicates_file.name}")
            
            with pd.ExcelWriter(duplicates_file, engine='openpyxl') as writer:
                # Sheet 1: All duplicates with full details
                export_cols = [
                    'Season', 'Race', 'OwnerNumber', 'OwnerName', 'OwnerQatariID',
                    'AwardAmount', 'EntryDate', 'PaymentReference', 'PaymentReference_D1',
                    '_DuplicateGroup', '_DuplicateCount', '_DuplicateSeverity',
                    'SourceFile'
                ]
                available_cols = [col for col in export_cols if col in self.duplicates.columns]
                
                self.duplicates[available_cols].to_excel(
                    writer, 
                    sheet_name='All_Duplicates',
                    index=False
                )
                
                # Sheet 2: Summary by duplicate group
                summary = self.duplicates.groupby('_DuplicateGroup').agg({
                    'AwardAmount': ['first', 'sum', 'count'],
                    'EntryDate': ['min', 'max'] if 'EntryDate' in self.duplicates.columns else ['count'],
                    'OwnerName': 'first',
                    'OwnerNumber': 'first',
                    'Race': 'first',
                    'Season': 'first'
                }).reset_index()
                
                summary.columns = ['GroupID', 'Amount', 'TotalAmount', 'Count', 
                                 'FirstDate', 'LastDate', 'Owner', 'OwnerNumber', 'Race', 'Season']
                summary = summary.sort_values('Count', ascending=False)
                
                summary.to_excel(writer, sheet_name='Summary_by_Group', index=False)
                
                # Sheet 3: Composite key breakdown
                key_analysis = pd.DataFrame({
                    'Field': ['Season', 'Race', 'OwnerNumber', 'OwnerName', 'OwnerQatariID', 'AwardAmount'],
                    'Purpose': [
                        'ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙˆØ³Ù…',
                        'ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø³Ø¨Ø§Ù‚',
                        'Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ù„Ùƒ Ø§Ù„ÙØ±ÙŠØ¯',
                        'Ø§Ø³Ù… Ø§Ù„Ù…Ø§Ù„Ùƒ Ø§Ù„ÙƒØ§Ù…Ù„',
                        'Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù‚Ø·Ø±ÙŠ Ù„Ù„Ù…Ø§Ù„Ùƒ',
                        'Ù…Ø¨Ù„Øº Ø§Ù„Ø¬Ø§Ø¦Ø²Ø©'
                    ],
                    'Matching': ['100%', '100%', '100%', '100%', '100%', '100%']
                })
                
                key_analysis.to_excel(writer, sheet_name='Composite_Key_Info', index=False)
            
            print(f"   âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸: {len(self.duplicates):,} Ø³Ø¬Ù„ Ù…ÙƒØ±Ø±")
            generated_files['duplicates'] = str(duplicates_file)
        
        return generated_files
