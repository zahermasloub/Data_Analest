# -*- coding: utf-8 -*-
"""
Fast File Loader - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ©
"""

from __future__ import annotations

from pathlib import Path
from typing import List, Tuple, Dict, Any
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import io


def _quick_clean_dataframe(df: pd.DataFrame, label: str = "") -> Tuple[pd.DataFrame, List[str]]:
    """ØªÙ†Ø¸ÙŠÙ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù€ DataFrame - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ø£Ø¯Ø§Ø¡
    
    Args:
        df: DataFrame Ù„Ù„ØªÙ†Ø¸ÙŠÙ
        label: ØªØ³Ù…ÙŠØ© ØªÙˆØ¶ÙŠØ­ÙŠØ©
        
    Returns:
        (cleaned_df, warnings): DataFrame Ù†Ø¸ÙŠÙ ÙˆÙ‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª
    """
    warnings = []
    
    # 1. Ø¥Ø²Ø§Ù„Ø© Ø£Ø¹Ù…Ø¯Ø© Unnamed (Ø¨Ø­Ø« Ø£Ø³Ø±Ø¹)
    unnamed_mask = df.columns.str.lower().str.contains('unnamed', na=False)
    if unnamed_mask.any():
        unnamed_cols = df.columns[unnamed_mask].tolist()
        df = df.drop(columns=unnamed_cols)
        warnings.append(f"ğŸ—‘ï¸ Ø­Ø°Ù {len(unnamed_cols)} Ø¹Ù…ÙˆØ¯ Unnamed")
    
    # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø© (Ø¨Ø¯ÙˆÙ† Ø­Ù„Ù‚Ø§Øª)
    if df.columns.duplicated().any():
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ÙˆÙ„ ÙÙ‚Ø· Ù…Ù† ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø§Øª
        df = df.loc[:, ~df.columns.duplicated(keep='first')]
        warnings.append(f"âš ï¸ Ø¥Ø²Ø§Ù„Ø© Ø£Ø¹Ù…Ø¯Ø© Ù…ÙƒØ±Ø±Ø©")
    
    return df, warnings


def _read_file_fast(file_obj, name: str) -> Tuple[List[pd.DataFrame], List[Dict[str, Any]]]:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø¨Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ©
    
    Args:
        file_obj: ÙƒØ§Ø¦Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹
        name: Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
        
    Returns:
        (dfs, stats): Ù‚Ø§Ø¦Ù…Ø© DataFrames ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    """
    suffix = Path(name).suffix.lower()
    dfs: List[pd.DataFrame] = []
    stats: List[Dict[str, Any]] = []
    
    try:
        file_obj.seek(0)
    except Exception:
        pass
    
    try:
        if suffix in [".csv", ".txt"]:
            # Ù‚Ø±Ø§Ø¡Ø© CSV Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
            df = pd.read_csv(
                file_obj,
                low_memory=False,  # ØªØ¬Ù†Ø¨ ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ù†ÙˆØ¹
                engine='c',  # Ù…Ø­Ø±Ùƒ C Ø£Ø³Ø±Ø¹
                encoding_errors='ignore'  # ØªØ¬Ø§Ù‡Ù„ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªØ±Ù…ÙŠØ²
            )
            df, warnings = _quick_clean_dataframe(df, name)
            dfs.append(df)
            stats.append({
                "label": name,
                "rows": len(df),
                "columns": len(df.columns),
                "warnings": warnings
            })
            
        elif suffix in [".xlsx", ".xls"]:
            # Ù‚Ø±Ø§Ø¡Ø© Excel Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª
            excel = pd.read_excel(
                file_obj,
                sheet_name=None,
                engine='openpyxl'
            )
            
            for sheet_name, df in excel.items():
                label = f"{name}::{sheet_name}"
                df, warnings = _quick_clean_dataframe(df, label)
                dfs.append(df)
                stats.append({
                    "label": label,
                    "rows": len(df),
                    "columns": len(df.columns),
                    "warnings": warnings
                })
        else:
            raise ValueError(f"ØµÙŠØºØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©: {suffix}")
            
    except Exception as e:
        stats.append({
            "label": name,
            "rows": 0,
            "columns": 0,
            "warnings": [f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}"]
        })
    
    return dfs, stats


def load_files_parallel(
    uploaded_files: List[Any],
    use_duckdb: bool = True,
    drop_exact_duplicates: bool = True,
    max_workers: int = 4
) -> Tuple[pd.DataFrame, List[Dict[str, Any]], int]:
    """ØªØ­Ù…ÙŠÙ„ Ù…ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª - Ø£Ø³Ø±Ø¹ Ù…Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„
    
    Args:
        uploaded_files: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©
        use_duckdb: Ø§Ø³ØªØ®Ø¯Ø§Ù… DuckDB Ù„Ù„Ø¯Ù…Ø¬
        drop_exact_duplicates: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
        max_workers: Ø¹Ø¯Ø¯ Ø§Ù„Ù€ threads Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
        
    Returns:
        (combined_df, per_part_stats, removed_duplicates_count)
    """
    if not uploaded_files:
        raise ValueError("Ù„Ù… ÙŠØªÙ… Ø±ÙØ¹ Ø£ÙŠ Ù…Ù„ÙØ§Øª")
    
    all_parts: List[pd.DataFrame] = []
    per_part_stats: List[Dict[str, Any]] = []
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„Ù…Ù„ÙØ§Øª
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ø§Ù… Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©
        future_to_file = {
            executor.submit(_read_file_fast, file_obj, getattr(file_obj, "name", f"file_{i}")): i
            for i, file_obj in enumerate(uploaded_files)
        }
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        for future in as_completed(future_to_file):
            try:
                dfs, stats = future.result()
                all_parts.extend(dfs)
                per_part_stats.extend(stats)
            except Exception as e:
                per_part_stats.append({
                    "label": f"unknown_file_{future_to_file[future]}",
                    "rows": 0,
                    "columns": 0,
                    "warnings": [f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: {str(e)}"]
                })
    
    if not all_parts:
        return pd.DataFrame(), per_part_stats, 0
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    combined: pd.DataFrame
    used_duckdb = False
    
    # ÙØ­Øµ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù…Ø®Ø·Ø·
    first_cols = set(all_parts[0].columns)
    same_schema = all(set(df.columns) == first_cols for df in all_parts)
    
    if use_duckdb and same_schema and len(all_parts) > 1:
        try:
            import duckdb
            con = duckdb.connect()
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
            for i, df in enumerate(all_parts):
                con.register(f"t{i}", df)
            
            # Ø¯Ù…Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… UNION ALL
            union_sql = " UNION ALL ".join([f"SELECT * FROM t{i}" for i in range(len(all_parts))])
            combined = con.execute(union_sql).df()
            con.close()
            used_duckdb = True
            
        except Exception:
            # Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ pandas Ø¥Ø°Ø§ ÙØ´Ù„ DuckDB
            combined = pd.concat(all_parts, ignore_index=True, sort=False, copy=False)
    else:
        # Ø¯Ù…Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pandas
        combined = pd.concat(all_parts, ignore_index=True, sort=False, copy=False)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
    removed = 0
    if drop_exact_duplicates and not combined.empty:
        before = len(combined)
        combined = combined.drop_duplicates().reset_index(drop=True)
        removed = before - len(combined)
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ø®Øµ
    per_part_stats.append({
        "label": "__summary__",
        "rows": len(combined),
        "columns": len(combined.columns),
        "used_duckdb": used_duckdb,
        "removed_exact_duplicates": removed,
    })
    
    return combined, per_part_stats, removed
