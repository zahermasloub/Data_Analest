"""
Enhanced Audit System - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
===========================================

Ù†Ø¸Ø§Ù… ØªØ¯Ù‚ÙŠÙ‚ Ø´Ø§Ù…Ù„ ÙˆÙ…Ø­Ø³Ù‘Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¨Øª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ

Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:
1. ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„ (DataNormalizer)
2. Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ù†ÙƒÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© (EnhancedBankMatcher)
3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© (GroundTruthValidator)
4. ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø´Ø§Ù…Ù„Ø© (ComprehensiveReportGenerator)

Ø§Ù„Ù…Ø¤Ù„Ù: Ù…Ø­Ù„Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ
Ø§Ù„ØªØ§Ø±ÙŠØ®: Ù†ÙˆÙÙ…Ø¨Ø± 2025
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Any, Optional
import re
import warnings

warnings.filterwarnings('ignore')


# ============================================================================
# Ø§Ù„Ù‚Ø³Ù… 1: DataNormalizer - Ù…Ø­ÙˆÙ‘Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯
# ============================================================================

class DataNormalizer:
    """
    Ù…Ø­ÙˆÙ‘Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯ - Data Normalization Engine
    
    Ø§Ù„ØºØ±Ø¶: ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆØ­Ù…Ø§ÙŠØ© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªØ¹Ø±ÙŠÙÙŠØ©
    
    Features:
    - Ø®Ø±ÙŠØ·Ø© ØªÙˆØ­ÙŠØ¯ Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø£Ø¹Ù…Ø¯Ø©
    - Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªØ¹Ø±ÙŠÙÙŠØ© Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù„Ù…ÙŠ
    - Ø¥Ø²Ø§Ù„Ø© Ø£Ø¹Ù…Ø¯Ø© Unnamed
    - ØªÙˆØ«ÙŠÙ‚ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø·Ø¨Ù‚
    """
    
    # Ø®Ø±ÙŠØ·Ø© ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©
    COLUMN_MAPPING = {
        # Entry Date
        'Entry Date': 'EntryDate',
        'EntryDate': 'EntryDate',
        'entry date': 'EntryDate',
        
        # Owner Information
        'Owner Number': 'OwnerNumber',
        'OwnerNumber': 'OwnerNumber',
        'owner number': 'OwnerNumber',
        
        'Owner Name': 'OwnerName',
        'OwnerName': 'OwnerName',
        'owner name': 'OwnerName',
        
        'Owner QatariId': 'OwnerQatariID',
        'OwnerQatariId': 'OwnerQatariID',
        'Owner Qatari Id': 'OwnerQatariID',
        'OwnerQatariID': 'OwnerQatariID',
        'owner qatariid': 'OwnerQatariID',
        
        # Award Information
        'Award Amount': 'AwardAmount',
        'AwardAmount': 'AwardAmount',
        'award amount': 'AwardAmount',
        
        # Payment Information
        'Payment Method': 'PaymentMethod',
        'PaymentType': 'PaymentMethod',
        'payment method': 'PaymentMethod',
        
        # Payment References (Ù…Ø¹ ØªØµØ­ÙŠØ­ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠ)
        'PaymentRefrence': 'PaymentReference',
        'PaymentReference': 'PaymentReference',
        'payment reference': 'PaymentReference',
        
        'PaymentRefrence_D1': 'PaymentReference_D1',
        'PaymentReference_D1': 'PaymentReference_D1',
        'DelegatePaymentReference': 'PaymentReference_D1',
        
        'PaymentRefrence_D2': 'PaymentReference_D2',
        'PaymentReference_D2': 'PaymentReference_D2',
        'SecondDelegatePaymentReference': 'PaymentReference_D2',
        
        'PaymentRefrence_D3': 'PaymentReference_D3',
        'PaymentReference_D3': 'PaymentReference_D3',
        'ThirdDelegatePaymentReference': 'PaymentReference_D3',
        
        # Beneficiary Information
        'Beneficiary English Name': 'BeneficiaryNameEn',
        'BeneficiaryEnglishName': 'BeneficiaryNameEn',
        'BeneficiaryNameEn': 'BeneficiaryNameEn',
        
        # IBAN
        'IBAN': 'IbanNumber',
        'Iban': 'IbanNumber',
        'IbanNumber': 'IbanNumber',
        
        # Transfer Information
        'Transfer Rate': 'TransferRate',
        'TransferRate': 'TransferRate',
        
        'Print Status': 'PrintStatus',
        'PrintStatus': 'PrintStatus',
        
        # Bank Statement Fields
        'Award Ref': 'AwardRef',
        'AwardRef': 'AwardRef',
        'Award Ref 10 Digits': 'AwardRef10Digits',
        'AwardRef10Digits': 'AwardRef10Digits',
        'Bank Reference': 'BankReference',
        'BankReference': 'BankReference',
        'Transfer Amount': 'TransferAmount',
        'TransferAmount': 'TransferAmount',
        'Transfer Date': 'TransferDate',
        'TransferDate': 'TransferDate',
        'Transaction Date': 'TransactionDate',
        'TransactionDate': 'TransactionDate',
        'Value Date': 'ValueDate',
        'ValueDate': 'ValueDate',
        'Beneficiary Name': 'BeneficiaryName',
        'BeneficiaryName': 'BeneficiaryName',
        'Currency Code': 'CurrencyCode',
        'CurrencyCode': 'CurrencyCode',
        'Debit': 'Debit',
        'Credit': 'Credit',
    }
    
    # Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªØ¹Ø±ÙŠÙÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø­Ù…Ø§ÙŠØªÙ‡Ø§
    ID_FIELDS = [
        'OwnerQatariID',
        'TrainerQatariId',
        'OwnerNumber',
        'JockeyQatariId',
    ]
    
    def __init__(self):
        self.applied_mappings = []
        self.removed_columns = []
        self.protected_fields = []
    
    def normalize_dataframe(self, df: pd.DataFrame, df_name: str = "DataFrame") -> pd.DataFrame:
        """
        ØªÙˆØ­ÙŠØ¯ DataFrame Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„
        
        Args:
            df: DataFrame Ù„Ù„ØªÙˆØ­ÙŠØ¯
            df_name: Ø§Ø³Ù… Ù„Ù„ØªÙˆØ«ÙŠÙ‚
            
        Returns:
            DataFrame Ù…ÙˆØ­Ø¯
        """
        df = df.copy()
        original_columns = df.columns.tolist()
        
        print(f"\nğŸ”§ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {df_name}")
        print(f"   Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©: {len(original_columns)}")
        
        # 0. **Ø­Ù…Ø§ÙŠØ© Ù…Ø³Ø¨Ù‚Ø© Ø´Ø§Ù…Ù„Ø©**: Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ø£Ø¹Ù…Ø¯Ø© Unnamed (case-insensitive check)
        unnamed_cols_strict = [col for col in df.columns if 'unnamed' in str(col).lower()]
        if unnamed_cols_strict:
            df = df.drop(columns=unnamed_cols_strict)
            self.removed_columns.extend(unnamed_cols_strict)
            print(f"   ğŸ”´ Ø­Ù…Ø§ÙŠØ© Ø·Ø§Ø±Ø¦Ø©: Ø­Ø°Ù {len(unnamed_cols_strict)} Ø¹Ù…ÙˆØ¯ Unnamed: {unnamed_cols_strict}")
        
        # 1. Ø¥Ø²Ø§Ù„Ø© Ø£Ø¹Ù…Ø¯Ø© Unnamed (legacy check with .startswith)
        unnamed_cols = [col for col in df.columns if str(col).startswith('Unnamed')]
        if unnamed_cols:
            df = df.drop(columns=unnamed_cols)
            self.removed_columns.extend(unnamed_cols)
            print(f"   âœ… Ø¥Ø²Ø§Ù„Ø© {len(unnamed_cols)} Ø¹Ù…ÙˆØ¯ Unnamed")
        
        # 2. ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        df.columns = df.columns.str.strip()
        
        # 2.5. **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªÙˆØ­ÙŠØ¯**
        if df.columns.duplicated().any():
            duplicated_cols = df.columns[df.columns.duplicated(keep=False)].unique().tolist()
            print(f"   âš ï¸ Ø£Ø¹Ù…Ø¯Ø© Ù…ÙƒØ±Ø±Ø© Ù…ÙƒØªØ´ÙØ©: {duplicated_cols}")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ù…Ø§Ø¡ Ø£Ø¹Ù…Ø¯Ø© ÙØ±ÙŠØ¯Ø© Ø¨Ø¥Ø¶Ø§ÙØ© Ø±Ù‚Ù… ØªØ³Ù„Ø³Ù„ÙŠ
            new_cols = []
            col_counts = {}
            for col in df.columns:
                if col in col_counts:
                    col_counts[col] += 1
                    new_cols.append(f"{col}_dup{col_counts[col]}")
                else:
                    col_counts[col] = 0
                    new_cols.append(col)
            
            df.columns = new_cols
            print(f"   ğŸ”§ Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø¨Ø¥Ø¶Ø§ÙØ© _dup suffix")
        
        # 3. ØªØ·Ø¨ÙŠÙ‚ Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØªÙˆØ­ÙŠØ¯
        mappings_applied = {}
        for col in df.columns:
            if col in self.COLUMN_MAPPING:
                new_name = self.COLUMN_MAPPING[col]
                if col != new_name:
                    mappings_applied[col] = new_name
                    self.applied_mappings.append({
                        'DataFrame': df_name,
                        'Original': col,
                        'Unified': new_name
                    })
        
        if mappings_applied:
            df = df.rename(columns=mappings_applied)
            print(f"   âœ… ØªÙˆØ­ÙŠØ¯ {len(mappings_applied)} Ø¹Ù…ÙˆØ¯")
        
        # 4. Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø§Ù„ØªØ¹Ø±ÙŠÙÙŠØ©
        for field in self.ID_FIELDS:
            if field in df.columns:
                try:
                    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† field Ù‡Ùˆ string ÙˆÙ„ÙŠØ³ list/tuple
                    if not isinstance(field, str):
                        print(f"   âš ï¸ ØªØ®Ø·ÙŠ Ø­Ù‚Ù„ ØºÙŠØ± ØµØ§Ù„Ø­: {field} (Ù†ÙˆØ¹Ù‡: {type(field)})")
                        continue
                    
                    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† df[field] ÙŠØ±Ø¬Ø¹ Series ÙˆÙ„ÙŠØ³ DataFrame
                    col_data = df[field]
                    if isinstance(col_data, pd.DataFrame):
                        print(f"   âš ï¸ ØªØ®Ø·ÙŠ {field}: ÙŠØ±Ø¬Ø¹ DataFrame Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Series")
                        continue
                    
                    # ØªØ­ÙˆÙŠÙ„ Ù„Ù†Øµ ÙˆØ¥Ø²Ø§Ù„Ø© .0 Ø¥Ù† ÙˆØ¬Ø¯Øª
                    df[field] = col_data.astype(str).str.replace('.0', '', regex=False).str.strip()
                    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ nan/None Ø¨Ù‚ÙŠÙ…Ø© ÙØ§Ø±ØºØ©
                    df[field] = df[field].replace(['nan', 'None', 'NaN'], '')
                    self.protected_fields.append(field)
                except Exception as e:
                    print(f"   âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ù‚Ù„ {field}: {str(e)}")
                    continue
        
        if self.protected_fields:
            print(f"   ğŸ”’ Ø­Ù…Ø§ÙŠØ© {len(set(self.protected_fields))} Ø­Ù‚Ù„ Ø±Ù‚Ù…ÙŠ")
        
        # 5. ØªÙ†Ø¸ÙŠÙ Ù‚ÙŠÙ… Ù†ØµÙŠØ© Ø¹Ø§Ù…Ø©
        text_fields = ['Season', 'Race', 'OwnerName']
        for field in text_fields:
            if field in df.columns:
                try:
                    col_data = df[field]
                    if isinstance(col_data, pd.DataFrame):
                        print(f"   âš ï¸ ØªØ®Ø·ÙŠ {field}: ÙŠØ±Ø¬Ø¹ DataFrame Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Series")
                        continue
                    df[field] = col_data.astype(str).str.strip()
                    df[field] = df[field].apply(lambda x: ' '.join(str(x).split()))  # Ø¯Ù…Ø¬ Ù…Ø³Ø§ÙØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
                except Exception as e:
                    print(f"   âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ {field}: {str(e)}")
                    continue
        
        # 6. ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ù„Øº
        amount_fields = ['AwardAmount', 'TransferAmount', 'Debit', 'Credit']
        for field in amount_fields:
            if field in df.columns:
                try:
                    col_data = df[field]
                    if isinstance(col_data, pd.DataFrame):
                        print(f"   âš ï¸ ØªØ®Ø·ÙŠ {field}: ÙŠØ±Ø¬Ø¹ DataFrame Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Series")
                        continue
                    # Ø¥Ø²Ø§Ù„Ø© Ø±Ù…ÙˆØ² Ø§Ù„Ø¹Ù…Ù„Ø© ÙˆØ§Ù„ÙÙˆØ§ØµÙ„
                    df[field] = col_data.astype(str).str.replace(r'[^\d.-]', '', regex=True)
                    df[field] = pd.to_numeric(df[field], errors='coerce')
                    df[field] = df[field].round(2)
                except Exception as e:
                    print(f"   âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¨Ù„Øº {field}: {str(e)}")
                    continue
        
        # 7. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        date_fields = ['EntryDate', 'TransferDate', 'TransactionDate', 'ValueDate']
        for field in date_fields:
            if field in df.columns:
                try:
                    col_data = df[field]
                    if isinstance(col_data, pd.DataFrame):
                        print(f"   âš ï¸ ØªØ®Ø·ÙŠ {field}: ÙŠØ±Ø¬Ø¹ DataFrame Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Series")
                        continue
                    df[field] = pd.to_datetime(col_data, errors='coerce')
                except Exception as e:
                    print(f"   âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ® {field}: {str(e)}")
                    continue
        
        print(f"   âœ… Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯: {len(df.columns)}")
        
        return df
    
    def get_mapping_documentation(self) -> pd.DataFrame:
        """
        ØªÙˆØ«ÙŠÙ‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙˆØ­ÙŠØ¯Ø§Øª Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©
        
        Returns:
            DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØªÙˆØ­ÙŠØ¯
        """
        if not self.applied_mappings:
            return pd.DataFrame({
                'DataFrame': ['Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆØ­ÙŠØ¯Ø§Øª'],
                'Original': [''],
                'Unified': ['']
            })
        
        return pd.DataFrame(self.applied_mappings)
    
    def get_full_mapping_table(self) -> pd.DataFrame:
        """
        Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ (Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…ØªØ§Ø­Ø©)
        
        Returns:
            DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…Ø¹Ø±Ù‘ÙØ©
        """
        mappings = []
        for original, unified in self.COLUMN_MAPPING.items():
            if original != unified:  # ÙÙ‚Ø· Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„ØªÙŠ ØªÙØºÙŠÙ‘Ø± Ø§Ù„Ø§Ø³Ù…
                mappings.append({
                    'Original_Name': original,
                    'Unified_Name': unified,
                    'Category': self._categorize_field(unified)
                })
        
        return pd.DataFrame(mappings)
    
    def _categorize_field(self, field_name: str) -> str:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø­Ù‚Ù„ Ø­Ø³Ø¨ Ù†ÙˆØ¹Ù‡"""
        if 'Owner' in field_name:
            return 'Owner Information'
        elif 'Award' in field_name or 'Amount' in field_name:
            return 'Award Information'
        elif 'Payment' in field_name or 'Reference' in field_name:
            return 'Payment Information'
        elif 'Bank' in field_name or 'Transfer' in field_name:
            return 'Bank Information'
        elif 'Date' in field_name:
            return 'Date Fields'
        else:
            return 'Other'


# ============================================================================
# Ø§Ù„Ù‚Ø³Ù… 2: EnhancedBankMatcher - Ù…Ø·Ø§Ø¨Ù‚ Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# ============================================================================

class EnhancedBankMatcher:
    """
    Ù…Ø·Ø§Ø¨Ù‚ Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Enhanced Bank Matcher
    
    Ø§Ù„ØºØ±Ø¶: Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ø¨ÙŠÙ† Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¬ÙˆØ§Ø¦Ø² ÙˆÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ
    
    Features:
    - Ø¯Ø¹Ù… Ø¬Ù…ÙŠØ¹ Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¯ÙØ¹ (PaymentReference, D1, D2, D3)
    - Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø±Ù†Ø© (Ø¢Ø®Ø± N Ø£Ø±Ù‚Ø§Ù…)
    - ÙØ­Øµ Ø§Ù„Ù…Ø¨Ù„Øº ÙˆØ§Ù„ØªØ§Ø±ÙŠØ®
    - ØªØµÙ†ÙŠÙ Ø«Ù„Ø§Ø«ÙŠ: Matched / Partial / Unmatched
    """
    
    def __init__(
        self,
        ref_last_digits: int = 10,
        amount_tolerance: float = 0.00,
        date_window_days: int = 14
    ):
        """
        Args:
            ref_last_digits: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø±Ù†Ø©
            amount_tolerance: Ø§Ù„ØªØ³Ø§Ù…Ø­ ÙÙŠ Ø§Ù„Ù…Ø¨Ù„Øº (0.00 = ØªØ·Ø§Ø¨Ù‚ ØªØ§Ù…)
            date_window_days: Ù†Ø§ÙØ°Ø© Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø© (Â±days)
        """
        self.ref_last_digits = ref_last_digits
        self.amount_tolerance = amount_tolerance
        self.date_window_days = date_window_days
        
        self.matched_records = []
        self.partial_records = []
        self.unmatched_records = []
    
    def match_awards_to_bank(
        self,
        awards_df: pd.DataFrame,
        bank_df: pd.DataFrame
    ) -> Dict[str, pd.DataFrame]:
        """
        Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¬Ù…ÙŠØ¹ Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¬ÙˆØ§Ø¦Ø² Ù…Ø¹ ÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ
        
        Args:
            awards_df: DataFrame Ø§Ù„Ø¬ÙˆØ§Ø¦Ø²
            bank_df: DataFrame Ø§Ù„Ø¨Ù†Ùƒ
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰: matched, partial, unmatched
        """
        print("\n" + "="*80)
        print("ğŸ¦ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
        print("="*80)
        print(f"   ğŸ“‹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©:")
        print(f"      â€¢ Ø¢Ø®Ø± {self.ref_last_digits} Ø£Ø±Ù‚Ø§Ù… Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø±Ù†Ø©")
        print(f"      â€¢ Ø§Ù„ØªØ³Ø§Ù…Ø­ ÙÙŠ Ø§Ù„Ù…Ø¨Ù„Øº: {self.amount_tolerance:.2f} Ø±ÙŠØ§Ù„")
        print(f"      â€¢ Ù†Ø§ÙØ°Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®: Â±{self.date_window_days} ÙŠÙˆÙ…")
        
        # ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ù†Ùƒ
        bank_df = self._prepare_bank_data(bank_df)
        
        total = len(awards_df)
        print(f"\nğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© {total:,} Ø³Ø¬Ù„...")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø³Ø¬Ù„
        for idx, row in awards_df.iterrows():
            match_result = self._match_single_award(row, bank_df)
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            if match_result['status'] == 'MATCHED_100':
                self.matched_records.append(match_result['data'])
            elif match_result['status'] == 'PARTIAL':
                self.partial_records.append(match_result['data'])
            else:
                self.unmatched_records.append(match_result['data'])
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ DataFrames
        matched_df = pd.DataFrame(self.matched_records) if self.matched_records else pd.DataFrame()
        partial_df = pd.DataFrame(self.partial_records) if self.partial_records else pd.DataFrame()
        unmatched_df = pd.DataFrame(self.unmatched_records) if self.unmatched_records else pd.DataFrame()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self._print_statistics(matched_df, partial_df, unmatched_df, total)
        
        return {
            'matched': matched_df,
            'partial': partial_df,
            'unmatched': unmatched_df
        }
    
    def _prepare_bank_data(self, bank_df: pd.DataFrame) -> pd.DataFrame:
        """ØªØ­Ø¶ÙŠØ± ÙˆØªÙ†Ø¸ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ù†Ùƒ"""
        bank_df = bank_df.copy()
        
        # ØªÙ†Ø¸ÙŠÙ Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
        ref_fields = ['AwardRef', 'AwardRef10Digits', 'BankReference']
        for field in ref_fields:
            if field in bank_df.columns:
                bank_df[field] = bank_df[field].astype(str).str.strip()
                bank_df[field] = bank_df[field].str.replace(r'[^\w]', '', regex=True)
                bank_df[field] = bank_df[field].str.lower()
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø¨Ù„Øº Ø±Ù‚Ù…ÙŠ
        if 'TransferAmount' in bank_df.columns:
            bank_df['TransferAmount'] = pd.to_numeric(bank_df['TransferAmount'], errors='coerce')
        
        return bank_df
    
    def _extract_all_references(self, row: pd.Series) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ù…Ù† Ø³Ø¬Ù„ Ø¬Ø§Ø¦Ø²Ø©"""
        refs = []
        ref_fields = ['PaymentReference', 'PaymentReference_D1', 'PaymentReference_D2', 'PaymentReference_D3']
        
        for field in ref_fields:
            try:
                if field in row.index:
                    value = row[field]
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ…Ø© Ù„ÙŠØ³Øª Series
                    if isinstance(value, pd.Series):
                        continue
                    if pd.notna(value) and value is not None:
                        ref = str(value).strip()
                        if ref and ref.lower() not in ['nan', 'none', '']:
                            refs.append(ref)
            except:
                continue
        
        return refs
    
    def _clean_reference(self, ref: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø±Ù‚Ù… Ù…Ø±Ø¬Ø¹ÙŠ"""
        ref = str(ref).strip()
        ref = re.sub(r'[^\w]', '', ref)  # Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„ Ù…Ø§ Ø¹Ø¯Ø§ Ø­Ø±ÙˆÙ ÙˆØ£Ø±Ù‚Ø§Ù…
        ref = ref.lower()
        return ref
    
    def _match_single_award(self, award_row: pd.Series, bank_df: pd.DataFrame) -> Dict:
        """Ù…Ø·Ø§Ø¨Ù‚Ø© Ø³Ø¬Ù„ Ø¬Ø§Ø¦Ø²Ø© ÙˆØ§Ø­Ø¯ Ù…Ø¹ Ø§Ù„Ø¨Ù†Ùƒ"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø§Ø¦Ø²Ø©
        award_data = award_row.to_dict()
        award_refs = self._extract_all_references(award_row)
        award_amount = award_row.get('AwardAmount', 0)
        award_date = award_row.get('EntryDate')
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø±Ø§Ø¬Ø¹
        if not award_refs:
            award_data['MatchStatus'] = 'NO_REFERENCE'
            award_data['MatchReason'] = 'âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ù‚Ù… Ù…Ø±Ø¬Ø¹ÙŠ'
            return {'status': 'UNMATCHED', 'data': award_data}
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¹ ÙƒÙ„ Ù…Ø±Ø¬Ø¹
        for ref in award_refs:
            ref_clean = self._clean_reference(ref)
            
            if len(ref_clean) < self.ref_last_digits:
                continue
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¨Ù†Ùƒ
            bank_matches = self._find_bank_matches(ref_clean, bank_df)
            
            if len(bank_matches) > 0:
                # ÙØ­Øµ Ø§Ù„Ù…Ø¨Ù„Øº
                amount_match = self._verify_amount(award_amount, bank_matches)
                
                if amount_match is not None:
                    # Ù…Ø·Ø§Ø¨Ù‚Ø© ÙƒØ§Ù…Ù„Ø© 100%
                    award_data['MatchStatus'] = 'MATCHED_100'
                    award_data['MatchReason'] = 'âœ… Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ù†ÙƒÙŠØ© ÙƒØ§Ù…Ù„Ø©'
                    award_data['MatchedReference'] = ref
                    award_data['BankTransferAmount'] = amount_match.get('TransferAmount')
                    award_data['BankTransactionDate'] = amount_match.get('TransactionDate')
                    award_data['BankValueDate'] = amount_match.get('ValueDate')
                    award_data['BankBeneficiary'] = amount_match.get('BeneficiaryName')
                    award_data['BankReference'] = amount_match.get('BankReference')
                    award_data['BankIBAN'] = amount_match.get('IBAN')
                    award_data['AmountDifference'] = 0.00
                    
                    # ÙØ­Øµ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
                    if award_date and pd.notna(award_date):
                        date_check = self._verify_date(award_date, amount_match)
                        award_data['DateCheck'] = date_check
                    
                    return {'status': 'MATCHED_100', 'data': award_data}
                else:
                    # Ù…Ø±Ø¬Ø¹ Ù…Ø·Ø§Ø¨Ù‚ Ù„ÙƒÙ† Ù…Ø¨Ù„Øº Ù…Ø®ØªÙ„Ù = PARTIAL
                    best_match = bank_matches.iloc[0]
                    bank_amount = best_match.get('TransferAmount', 0)
                    diff = abs(award_amount - bank_amount) if bank_amount else 0
                    
                    award_data['MatchStatus'] = 'PARTIAL'
                    award_data['MatchReason'] = f'âš ï¸ Ref Ù…Ø·Ø§Ø¨Ù‚ - Ù…Ø¨Ù„Øº Ù…Ø®ØªÙ„Ù (ÙØ±Ù‚: {diff:.2f})'
                    award_data['MatchedReference'] = ref
                    award_data['BankTransferAmount'] = bank_amount
                    award_data['AmountDifference'] = diff
                    award_data['BankReference'] = best_match.get('BankReference')
                    
                    return {'status': 'PARTIAL', 'data': award_data}
        
        # Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø·Ø§Ø¨Ù‚Ø©
        award_data['MatchStatus'] = 'UNMATCHED'
        award_data['MatchReason'] = 'âŒ Ref ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„Ø¨Ù†Ùƒ'
        award_data['AttemptedRefs'] = ' | '.join(award_refs)
        
        return {'status': 'UNMATCHED', 'data': award_data}
    
    def _find_bank_matches(self, ref_clean: str, bank_df: pd.DataFrame) -> pd.DataFrame:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø·Ø§Ø¨Ù‚Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ù†Ùƒ"""
        ref_last = ref_clean[-self.ref_last_digits:]
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù…ÙƒÙ†Ø©
        matches = pd.DataFrame()
        
        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹
        possible_ref_columns = ['AwardRef', 'AwardRef10Digits', 'BankReference', 'PaymentReference']
        
        for col in possible_ref_columns:
            if col in bank_df.columns:
                col_matches = bank_df[
                    bank_df[col].astype(str).str.contains(ref_last, na=False, case=False)
                ]
                if len(col_matches) > 0:
                    matches = pd.concat([matches, col_matches], ignore_index=True)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        if len(matches) > 0:
            matches = matches.drop_duplicates()
        
        return matches
    
    def _verify_amount(self, award_amount: float, bank_matches: pd.DataFrame) -> Optional[Dict]:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø¨Ù„Øº"""
        for idx, bank_row in bank_matches.iterrows():
            bank_amount = bank_row.get('TransferAmount')
            
            if pd.isna(bank_amount):
                continue
            
            diff = abs(award_amount - bank_amount)
            
            if diff <= self.amount_tolerance:
                return bank_row.to_dict()
        
        return None
    
    def _verify_date(self, award_date: pd.Timestamp, bank_row: Dict) -> str:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ§Ø±ÙŠØ®"""
        bank_date = bank_row.get('TransactionDate') or bank_row.get('ValueDate')
        
        if pd.isna(bank_date):
            return 'ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ù†Ùƒ ØºÙŠØ± Ù…ØªØ§Ø­'
        
        diff_days = abs((award_date - bank_date).days)
        
        if diff_days <= self.date_window_days:
            return f'âœ… Ø¶Ù…Ù† Ø§Ù„Ù†Ø§ÙØ°Ø© ({diff_days} ÙŠÙˆÙ…)'
        else:
            return f'âš ï¸ Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø§ÙØ°Ø© ({diff_days} ÙŠÙˆÙ…)'
    
    def _print_statistics(self, matched, partial, unmatched, total):
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        matched_count = len(matched)
        partial_count = len(partial)
        unmatched_count = len(unmatched)
        
        match_rate = (matched_count / total * 100) if total > 0 else 0
        partial_rate = (partial_count / total * 100) if total > 0 else 0
        
        print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©:")
        print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª: {total:,}")
        print(f"   âœ… Ù…Ø·Ø§Ø¨Ù‚ 100%: {matched_count:,} ({match_rate:.1f}%)")
        print(f"   âš ï¸ Ø¬Ø²Ø¦ÙŠ/Ù…Ø´ØªØ¨Ù‡: {partial_count:,} ({partial_rate:.1f}%)")
        print(f"   âŒ ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚: {unmatched_count:,} ({100-match_rate-partial_rate:.1f}%)")
        
        # ØªÙØµÙŠÙ„ Ø£Ø³Ø¨Ø§Ø¨ Ø¹Ø¯Ù… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
        if unmatched_count > 0 and len(unmatched) > 0:
            print(f"\nâš ï¸ Ø£Ø³Ø¨Ø§Ø¨ Ø¹Ø¯Ù… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©:")
            if 'MatchReason' in unmatched.columns:
                reasons = unmatched['MatchReason'].value_counts()
                for reason, count in reasons.items():
                    print(f"   â€¢ {reason}: {count:,} Ø³Ø¬Ù„")


# ============================================================================
# Ø§Ù„Ù‚Ø³Ù… 3: GroundTruthValidator - Ù…Ø¯Ù‚Ù‚ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
# ============================================================================

class GroundTruthValidator:
    """
    Ù…Ø¯Ù‚Ù‚ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© - Ground Truth Validator
    
    Ø§Ù„ØºØ±Ø¶: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ÙƒØªØ´Ø§Ù Ø¬Ù…ÙŠØ¹ Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ù…Ø³Ø¨Ù‚Ø§Ù‹
    
    Features:
    - 28 Ø­Ø§Ù„Ø© Ù…ÙØ­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
    - ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ø¨Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©/Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
    - Ù†Ø³Ø¨Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
    """
    
    # Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ù…Ù† Ø§Ù„Ø¨Ø±Ù…Ø¨Øª (Ground Truth)
    KNOWN_DUPLICATE_PAIRS = [
        ('821B291050', '821B291373'),
        ('821B731113', '822B731638'),
        ('821B731181', '822B731655'),
        ('821B780256', '821B780936'),
        ('821B780108', '822B780961'),
        ('822B860645', '822B861016', '822B861020'),  # Ø«Ù„Ø§Ø«ÙŠ
        ('822B870766', '822B871164'),
        ('822C161124', '824C161706'),
        ('822C160320', '824C161708'),
        ('822C160718', '824C161711'),
        ('822C160760', '822C161657'),
        ('822C220292', '823C220693'),
        ('822C340338', '823C341078'),
        ('823C360755', '823C361534'),
        ('823C360159', '823C361544'),
        ('823C360796', '823C361529'),
        ('822C340243', '823C341104', '823C341106'),  # Ø«Ù„Ø§Ø«ÙŠ
        ('824D101013', '824D101473'),
        ('823C360031', '824C361546', '823C361516'),  # Ø«Ù„Ø§Ø«ÙŠ
        ('820B150249', '823B150327'),
    ]
    
    def __init__(self):
        self.validation_results = None
    
    def validate_detection(self, duplicates_df: pd.DataFrame) -> Dict:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ÙƒØªØ´Ø§Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
        
        Args:
            duplicates_df: DataFrame Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚
        """
        print("\n" + "="*80)
        print("ğŸ” Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© (Ground Truth)")
        print("="*80)
        print(f"   ğŸ“‹ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©: {len(self.KNOWN_DUPLICATE_PAIRS)}")
        
        results = {
            'total_cases': len(self.KNOWN_DUPLICATE_PAIRS),
            'detected': [],
            'missing': [],
            'detection_rate': 0.0
        }
        
        for case_num, case_refs in enumerate(self.KNOWN_DUPLICATE_PAIRS, 1):
            detected, details = self._check_case_detected(case_refs, duplicates_df)
            
            case_info = {
                'case_number': case_num,
                'references': ' & '.join(case_refs),
                'ref_count': len(case_refs),
                'detected': detected,
                'details': details
            }
            
            if detected:
                results['detected'].append(case_info)
            else:
                results['missing'].append(case_info)
        
        detected_count = len(results['detected'])
        results['detection_rate'] = (detected_count / results['total_cases'] * 100) if results['total_cases'] > 0 else 0
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚:")
        print(f"   âœ… Ø­Ø§Ù„Ø§Øª Ù…ÙƒØªØ´ÙØ©: {detected_count} / {results['total_cases']}")
        print(f"   âŒ Ø­Ø§Ù„Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©: {len(results['missing'])} / {results['total_cases']}")
        print(f"   ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù: {results['detection_rate']:.1f}%")
        
        if results['missing']:
            print(f"\nâš ï¸ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©:")
            for case in results['missing'][:5]:  # Ø£ÙˆÙ„ 5 ÙÙ‚Ø·
                print(f"   â€¢ {case['references']}")
            if len(results['missing']) > 5:
                print(f"   ... Ùˆ {len(results['missing']) - 5} Ø­Ø§Ù„Ø© Ø£Ø®Ø±Ù‰")
        
        self.validation_results = results
        return results
    
    def _check_case_detected(self, case_refs: tuple, duplicates_df: pd.DataFrame) -> Tuple[bool, str]:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ÙƒØªØ´Ø§Ù Ø­Ø§Ù„Ø© Ù…Ø­Ø¯Ø¯Ø©
        
        Args:
            case_refs: tuple Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
            duplicates_df: DataFrame Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            
        Returns:
            (detected: bool, details: str)
        """
        ref_columns = ['PaymentReference', 'PaymentReference_D1', 'PaymentReference_D2', 'PaymentReference_D3']
        
        found_refs = set()
        
        for ref in case_refs:
            for col in ref_columns:
                if col in duplicates_df.columns:
                    try:
                        # Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ
                        mask = duplicates_df[col].astype(str).str.contains(ref, na=False, case=False)
                        matches = duplicates_df[mask]
                        
                        if len(matches) > 0:
                            found_refs.add(ref)
                            break
                    except:
                        continue
        
        # ÙŠØ¬Ø¨ Ø¥ÙŠØ¬Ø§Ø¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø©
        detected = len(found_refs) == len(case_refs)
        
        if detected:
            details = f'âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ({len(found_refs)}/{len(case_refs)}) Ù…ÙƒØªØ´ÙØ©'
        else:
            details = f'âŒ Ù…Ø±Ø§Ø¬Ø¹ Ù†Ø§Ù‚ØµØ© ({len(found_refs)}/{len(case_refs)}) - Ù…ÙÙ‚ÙˆØ¯: {set(case_refs) - found_refs}'
        
        return detected, details
    
    def generate_validation_report(self) -> pd.DataFrame:
        """
        Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ØªÙØµÙŠÙ„ÙŠ Ù„Ù„ØªØ­Ù‚Ù‚
        
        Returns:
            DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„ Ø­Ø§Ù„Ø© ÙˆØ­Ø§Ù„Ø© Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§
        """
        if not self.validation_results:
            return pd.DataFrame()
        
        all_cases = self.validation_results['detected'] + self.validation_results['missing']
        all_cases = sorted(all_cases, key=lambda x: x['case_number'])
        
        report_data = []
        for case in all_cases:
            report_data.append({
                'Case_Number': case['case_number'],
                'References': case['references'],
                'Reference_Count': case['ref_count'],
                'Status': 'âœ… Ù…ÙƒØªØ´Ù' if case['detected'] else 'âŒ Ù…ÙÙ‚ÙˆØ¯',
                'Details': case['details']
            })
        
        return pd.DataFrame(report_data)


# ============================================================================
# Ø§Ù„Ù‚Ø³Ù… 4: ComprehensiveReportGenerator - Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„
# ============================================================================

class ComprehensiveReportGenerator:
    """
    Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„ - Comprehensive Report Generator
    
    Ø§Ù„ØºØ±Ø¶: Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø­Ø³Ø¨ Ø§Ù„Ø¨Ø±Ù…Ø¨Øª
    
    Reports:
    1. Awards_Duplicates_[timestamp].xlsx
    2. Bank_Match_Verification_[timestamp].xlsx
    3. Ground_Truth_Validation_[timestamp].xlsx
    """
    
    def __init__(
        self,
        duplicates: pd.DataFrame,
        normalizer: DataNormalizer,
        bank_matcher: EnhancedBankMatcher,
        ground_truth_validator: GroundTruthValidator,
        output_dir: str = "outputs"
    ):
        self.duplicates = duplicates
        self.normalizer = normalizer
        self.bank_matcher = bank_matcher
        self.ground_truth_validator = ground_truth_validator
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Ø§Ù„ØªÙˆÙ‚ÙŠØª Ø­Ø³Ø¨ Ø§Ù„Ø¯ÙˆØ­Ø©
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    def generate_all_reports(self) -> Dict[str, str]:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
        
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø¨Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©
        """
        print("\n" + "="*80)
        print("ğŸ“„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„Ø©")
        print("="*80)
        
        generated_files = {}
        
        # Report 1: Duplicates
        dup_file = self.generate_duplicates_report()
        if dup_file:
            generated_files['duplicates'] = str(dup_file)
        
        # Report 2: Bank Verification
        bank_file = self.generate_bank_verification_report()
        if bank_file:
            generated_files['bank_verification'] = str(bank_file)
        
        # Report 3: Ground Truth
        gt_file = self.generate_ground_truth_report()
        if gt_file:
            generated_files['ground_truth'] = str(gt_file)
        
        return generated_files
    
    def generate_duplicates_report(self) -> Optional[Path]:
        """
        ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„
        
        Sheets:
        1. Duplicates_AllRows
        2. Duplicates_Summary
        3. Data_Dictionary
        """
        if self.duplicates is None or len(self.duplicates) == 0:
            print("   âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª Ù„Ù„ØªØµØ¯ÙŠØ±")
            return None
        
        file_path = self.output_dir / f"Awards_Duplicates_{self.timestamp}.xlsx"
        
        print(f"\nğŸ“ Ø§Ù„ØªÙ‚Ø±ÙŠØ± 1: ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª")
        print(f"   Ø§Ù„Ù…Ù„Ù: {file_path.name}")
        
        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            # Sheet 1: Duplicates_AllRows
            export_cols = [
                'Season', 'Race', 'OwnerNumber', 'OwnerName', 'OwnerQatariID',
                'AwardAmount', 'EntryDate',
                'PaymentReference', 'PaymentReference_D1', 'PaymentReference_D2', 'PaymentReference_D3',
                '_DuplicateGroup', '_DuplicateCount', '_DuplicateSeverity',
                'ReasonText', 'SourceFile'
            ]
            available = [c for c in export_cols if c in self.duplicates.columns]
            self.duplicates[available].to_excel(writer, sheet_name='Duplicates_AllRows', index=False)
            print(f"   âœ… Sheet 1: Duplicates_AllRows ({len(self.duplicates):,} Ø³Ø¬Ù„)")
            
            # Sheet 2: Duplicates_Summary
            if '_DuplicateGroup' in self.duplicates.columns:
                summary = self.duplicates.groupby('_DuplicateGroup').agg({
                    'AwardAmount': ['first', 'sum', 'count'],
                    'EntryDate': ['min', 'max'] if 'EntryDate' in self.duplicates.columns else ['count'],
                    'OwnerName': 'first',
                    'OwnerNumber': 'first',
                    'Race': 'first',
                    'Season': 'first'
                }).reset_index()
                
                summary.columns = ['GroupID', 'Amount', 'TotalAmount', 'Count', 
                                 'FirstDate', 'LastDate', 'Owner', 'OwnerNumber', 'Race', 'Season']
                summary = summary.sort_values('Count', ascending=False)
                
                summary.to_excel(writer, sheet_name='Duplicates_Summary', index=False)
                print(f"   âœ… Sheet 2: Duplicates_Summary ({len(summary):,} Ù…Ø¬Ù…ÙˆØ¹Ø©)")
            
            # Sheet 3: Data_Dictionary
            dictionary = self.normalizer.get_mapping_documentation()
            dictionary.to_excel(writer, sheet_name='Data_Dictionary', index=False)
            print(f"   âœ… Sheet 3: Data_Dictionary ({len(dictionary):,} ØªÙˆØ­ÙŠØ¯)")
        
        print(f"   âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸: {file_path.name}")
        return file_path
    
    def generate_bank_verification_report(self) -> Optional[Path]:
        """
        ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø¨Ù†ÙƒÙŠ Ø§Ù„Ø´Ø§Ù…Ù„
        
        Sheets:
        1. Bank_Matches
        2. Bank_PartialOrSuspected
        3. Bank_Unmatched
        4. Notes
        """
        matched = pd.DataFrame(self.bank_matcher.matched_records)
        partial = pd.DataFrame(self.bank_matcher.partial_records)
        unmatched = pd.DataFrame(self.bank_matcher.unmatched_records)
        
        if len(matched) == 0 and len(partial) == 0 and len(unmatched) == 0:
            print("   âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­Ù‚Ù‚ Ø¨Ù†ÙƒÙŠ")
            return None
        
        file_path = self.output_dir / f"Bank_Match_Verification_{self.timestamp}.xlsx"
        
        print(f"\nğŸ“ Ø§Ù„ØªÙ‚Ø±ÙŠØ± 2: ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø¨Ù†ÙƒÙŠ")
        print(f"   Ø§Ù„Ù…Ù„Ù: {file_path.name}")
        
        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            # Sheet 1: Bank_Matches
            if len(matched) > 0:
                matched.to_excel(writer, sheet_name='Bank_Matches', index=False)
                print(f"   âœ… Sheet 1: Bank_Matches ({len(matched):,} Ø³Ø¬Ù„)")
            
            # Sheet 2: Bank_PartialOrSuspected
            if len(partial) > 0:
                partial.to_excel(writer, sheet_name='Bank_PartialOrSuspected', index=False)
                print(f"   âš ï¸ Sheet 2: Bank_PartialOrSuspected ({len(partial):,} Ø³Ø¬Ù„)")
            
            # Sheet 3: Bank_Unmatched
            if len(unmatched) > 0:
                unmatched.to_excel(writer, sheet_name='Bank_Unmatched', index=False)
                print(f"   âŒ Sheet 3: Bank_Unmatched ({len(unmatched):,} Ø³Ø¬Ù„)")
            
            # Sheet 4: Notes
            notes = self._generate_notes()
            notes.to_excel(writer, sheet_name='Notes', index=False)
            print(f"   âœ… Sheet 4: Notes ({len(notes):,} Ù…Ù„Ø§Ø­Ø¸Ø©)")
        
        print(f"   âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸: {file_path.name}")
        return file_path
    
    def generate_ground_truth_report(self) -> Optional[Path]:
        """
        ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
        
        Sheets:
        1. Validation_Results
        2. Summary
        """
        if not self.ground_truth_validator.validation_results:
            print("   âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©")
            return None
        
        file_path = self.output_dir / f"Ground_Truth_Validation_{self.timestamp}.xlsx"
        
        print(f"\nğŸ“ Ø§Ù„ØªÙ‚Ø±ÙŠØ± 3: ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©")
        print(f"   Ø§Ù„Ù…Ù„Ù: {file_path.name}")
        
        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            # Sheet 1: Validation_Results
            report = self.ground_truth_validator.generate_validation_report()
            report.to_excel(writer, sheet_name='Validation_Results', index=False)
            print(f"   âœ… Sheet 1: Validation_Results ({len(report):,} Ø­Ø§Ù„Ø©)")
            
            # Sheet 2: Summary
            results = self.ground_truth_validator.validation_results
            summary = pd.DataFrame({
                'Metric': [
                    'Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©',
                    'Ø­Ø§Ù„Ø§Øª Ù…ÙƒØªØ´ÙØ©',
                    'Ø­Ø§Ù„Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©',
                    'Ù†Ø³Ø¨Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù'
                ],
                'Value': [
                    results['total_cases'],
                    len(results['detected']),
                    len(results['missing']),
                    f"{results['detection_rate']:.1f}%"
                ]
            })
            summary.to_excel(writer, sheet_name='Summary', index=False)
            print(f"   âœ… Sheet 2: Summary")
        
        print(f"   âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸: {file_path.name}")
        return file_path
    
    def _generate_notes(self) -> pd.DataFrame:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        notes_data = []
        
        # Settings
        notes_data.append({
            'Category': 'Settings',
            'Item': 'REF_LAST_DIGITS',
            'Value': str(self.bank_matcher.ref_last_digits),
            'Description': 'Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø±Ù†Ø©'
        })
        notes_data.append({
            'Category': 'Settings',
            'Item': 'AMOUNT_TOLERANCE',
            'Value': f"{self.bank_matcher.amount_tolerance:.2f}",
            'Description': 'Ø§Ù„ØªØ³Ø§Ù…Ø­ ÙÙŠ ÙØ±Ù‚ Ø§Ù„Ù…Ø¨Ù„Øº (0.00 = ØªØ·Ø§Ø¨Ù‚ ØªØ§Ù…)'
        })
        notes_data.append({
            'Category': 'Settings',
            'Item': 'DATE_WINDOW_DAYS',
            'Value': str(self.bank_matcher.date_window_days),
            'Description': 'Ù†Ø§ÙØ°Ø© Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø© (Â±Ø£ÙŠØ§Ù…)'
        })
        
        # Statistics
        notes_data.append({
            'Category': 'Statistics',
            'Item': 'Total Matched',
            'Value': str(len(self.bank_matcher.matched_records)),
            'Description': 'Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© 100%'
        })
        notes_data.append({
            'Category': 'Statistics',
            'Item': 'Total Partial',
            'Value': str(len(self.bank_matcher.partial_records)),
            'Description': 'Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¬Ø²Ø¦ÙŠØ©/Ø§Ù„Ù…Ø´ØªØ¨Ù‡Ø©'
        })
        notes_data.append({
            'Category': 'Statistics',
            'Item': 'Total Unmatched',
            'Value': str(len(self.bank_matcher.unmatched_records)),
            'Description': 'Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©'
        })
        
        # Assumptions
        notes_data.append({
            'Category': 'Assumptions',
            'Item': 'Reference Matching',
            'Value': 'Last 10 digits',
            'Description': 'Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø¢Ø®Ø± 10 Ø£Ø±Ù‚Ø§Ù… Ù…Ù† Ø§Ù„Ù…Ø±Ø¬Ø¹'
        })
        notes_data.append({
            'Category': 'Assumptions',
            'Item': 'Amount Matching',
            'Value': 'Exact match (0.00)',
            'Description': 'Ø§Ù„Ù…Ø¨Ù„Øº ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªØ·Ø§Ø¨Ù‚ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„'
        })
        notes_data.append({
            'Category': 'Assumptions',
            'Item': 'Multiple References',
            'Value': 'PaymentReference + D1/D2/D3',
            'Description': 'ÙŠØªÙ… ÙØ­Øµ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©'
        })
        
        return pd.DataFrame(notes_data)


# ============================================================================
# Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ - Enhanced Audit System
# ============================================================================

if __name__ == "__main__":
    print("="*80)
    print("Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† - Enhanced Audit System")
    print("="*80)
    print("\nÙ‡Ø°Ø§ Ù…Ù„Ù Ù…ÙƒØªØ¨Ø© (Library) - Ø§Ø³ØªØ®Ø¯Ù…Ù‡ Ù…Ù† Ø®Ù„Ø§Ù„:")
    print("  from core.enhanced_audit_system import DataNormalizer, EnhancedBankMatcher, ...")
    print("="*80)
