"""
ØªØ´ØºÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† - Run Enhanced Audit System
========================================================

Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ´ØºÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¨Øª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ

Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
- ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„
- ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø¯Ù‚Ø© 100%
- Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ù†ÙƒÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© (Matched / Partial / Unmatched)
- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© (28 Ø­Ø§Ù„Ø©)
- ØªÙ‚Ø§Ø±ÙŠØ± Ø´Ø§Ù…Ù„Ø© (3 Ù…Ù„ÙØ§Øª Excel)

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    python run_enhanced_audit.py
"""

from pathlib import Path
import sys
import pandas as pd
from datetime import datetime

# Fix console encoding
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except:
        pass

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent))

from core.enhanced_audit_system import (
    DataNormalizer,
    EnhancedBankMatcher,
    GroundTruthValidator,
    ComprehensiveReportGenerator
)


def load_awards_files(uploads_dir: Path) -> pd.DataFrame:
    """
    ØªØ­Ù…ÙŠÙ„ ÙˆØ¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬ÙˆØ§Ø¦Ø²
    
    Args:
        uploads_dir: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª
        
    Returns:
        DataFrame Ù…Ø¯Ù…ÙˆØ¬
    """
    print("\n" + "="*80)
    print("ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬ÙˆØ§Ø¦Ø²")
    print("="*80)
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„Ù Ù…Ø¯Ù…ÙˆØ¬
    combined_file = uploads_dir / "Combined_Awards_2018_2025.xlsx"
    
    if combined_file.exists():
        print(f"   ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¯Ù…ÙˆØ¬: {combined_file.name}")
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ù„Ù„Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù„Ù…ÙŠ
        dtype_dict = {
            'OwnerQatariId': str,
            'Owner Qatari Id': str,
            'OwnerQatariID': str,
            'OwnerNumber': str,
            'Owner Number': str,
            'TrainerQatariId': str
        }
        
        try:
            df = pd.read_excel(combined_file, dtype=dtype_dict)
            df['SourceFile'] = combined_file.name
            print(f"   âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„: {len(df):,} Ø³Ø¬Ù„")
            return df
        except Exception as e:
            print(f"   âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„: {e}")
            return pd.DataFrame()
    
    # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù Ù…Ø¯Ù…ÙˆØ¬ØŒ Ø§Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª ÙØ±Ø¯ÙŠØ©
    print("   ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª ÙØ±Ø¯ÙŠØ©...")
    award_files = list(uploads_dir.glob("Awards*.xlsx")) + list(uploads_dir.glob("AwardsForSeason*.xlsx"))
    
    if not award_files:
        print("   âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø¬ÙˆØ§Ø¦Ø²")
        return pd.DataFrame()
    
    print(f"   ğŸ“ Ø¹Ø«Ø± Ø¹Ù„Ù‰ {len(award_files)} Ù…Ù„Ù")
    
    all_dataframes = []
    
    for file in award_files:
        print(f"   ğŸ“„ {file.name}... ", end='')
        try:
            df = pd.read_excel(file, dtype=dtype_dict)
            df['SourceFile'] = file.name
            all_dataframes.append(df)
            print(f"âœ… ({len(df):,} Ø³Ø¬Ù„)")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£: {e}")
    
    if not all_dataframes:
        print("   âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª")
        return pd.DataFrame()
    
    # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
    merged = pd.concat(all_dataframes, ignore_index=True)
    print(f"\n   âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„Ø¯Ù…Ø¬: {len(merged):,}")
    
    return merged


def load_bank_statement(uploads_dir: Path) -> pd.DataFrame:
    """
    ØªØ­Ù…ÙŠÙ„ ÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ Ù…Ø¹ Ø§ÙƒØªØ´Ø§Ù ØªØ±ÙˆÙŠØ³Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠ
    
    Args:
        uploads_dir: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª
        
    Returns:
        DataFrame ÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ
    """
    print("\n" + "="*80)
    print("ğŸ¦ ØªØ­Ù…ÙŠÙ„ ÙƒØ´Ù Ø§Ù„Ø¨Ù†Ùƒ")
    print("="*80)
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„Ù Ø§Ù„Ø¨Ù†Ùƒ
    bank_files = list(uploads_dir.glob("*.csv")) + list(uploads_dir.glob("*Ø§Ù„Ø¨Ù†Ùƒ*.xlsx")) + list(uploads_dir.glob("*bank*.csv"))
    
    if not bank_files:
        print("   âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ø¨Ù†Ùƒ")
        return pd.DataFrame()
    
    # Ø£Ø®Ø° Ø£ÙˆÙ„ Ù…Ù„Ù
    bank_file = bank_files[0]
    print(f"   ğŸ“„ ØªØ­Ù…ÙŠÙ„: {bank_file.name}")
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© CSV
        if bank_file.suffix.lower() == '.csv':
            # ÙØ­Øµ Ø£ÙˆÙ„ 20 ØµÙ Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ÙˆÙŠØ³Ø©
            df_peek = pd.read_csv(bank_file, nrows=20, encoding='utf-8-sig', encoding_errors='ignore')
            
            header_row = None
            for i in range(len(df_peek)):
                row_values = df_peek.iloc[i].astype(str).str.lower()
                if any('award' in str(v).lower() or 'reference' in str(v).lower() for v in row_values):
                    header_row = i
                    print(f"   âœ… Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØªØ±ÙˆÙŠØ³Ø© ÙÙŠ Ø§Ù„ØµÙ: {i + 1}")
                    break
            
            if header_row is None:
                header_row = 0
                print(f"   âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙ Ø§Ù„Ø£ÙˆÙ„ ÙƒØªØ±ÙˆÙŠØ³Ø©")
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù ÙƒØ§Ù…Ù„Ø§Ù‹
            bank_df = pd.read_csv(bank_file, header=header_row, encoding='utf-8-sig', encoding_errors='ignore')
        else:
            # Ù‚Ø±Ø§Ø¡Ø© Excel
            bank_df = pd.read_excel(bank_file)
        
        # ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        bank_df.columns = bank_df.columns.str.strip()
        
        print(f"   âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„: {len(bank_df):,} Ù…Ø¹Ø§Ù…Ù„Ø©")
        print(f"   ğŸ“‹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {', '.join(bank_df.columns[:5].tolist())}...")
        
        return bank_df
        
    except Exception as e:
        print(f"   âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„: {e}")
        return pd.DataFrame()


def detect_duplicates_enhanced(
    df: pd.DataFrame,
    normalizer: DataNormalizer
) -> pd.DataFrame:
    """
    ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨
    
    Composite Key:
        Season + Race + OwnerNumber + OwnerName + OwnerQatariID + AwardAmount
    
    Args:
        df: DataFrame Ù…ÙˆØ­Ø¯
        normalizer: Ù…Ø­ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
    Returns:
        DataFrame Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    """
    print("\n" + "="*80)
    print("ğŸ” ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨")
    print("="*80)
    
    # Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    required_fields = ['Season', 'Race', 'OwnerNumber', 'OwnerName', 'OwnerQatariID', 'AwardAmount']
    
    print(f"   ğŸ“‹ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨ (6 Ø­Ù‚ÙˆÙ„):")
    for i, field in enumerate(required_fields, 1):
        print(f"      {i}. {field}")
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø­Ù‚ÙˆÙ„
    missing = [f for f in required_fields if f not in df.columns]
    if missing:
        print(f"   âŒ Ø­Ù‚ÙˆÙ„ Ù…ÙÙ‚ÙˆØ¯Ø©: {missing}")
        return pd.DataFrame()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø¨Ø­Ù‚ÙˆÙ„ ÙØ§Ø±ØºØ©
    df_clean = df.copy()
    initial_count = len(df_clean)
    
    for field in required_fields:
        df_clean = df_clean[df_clean[field].notna()].copy()
    
    removed = initial_count - len(df_clean)
    if removed > 0:
        print(f"   âš ï¸ Ø¥Ø²Ø§Ù„Ø© {removed:,} ØµÙ Ø¨Ø­Ù‚ÙˆÙ„ ÙØ§Ø±ØºØ©")
    
    # ØªÙ†Ø¸ÙŠÙ ÙˆØªØ·Ø¨ÙŠØ¹
    for field in ['Season', 'Race', 'OwnerName']:
        if field in df_clean.columns:
            df_clean[field] = df_clean[field].astype(str).str.strip().str.lower()
    
    df_clean['OwnerNumber'] = df_clean['OwnerNumber'].astype(str).str.strip()
    df_clean['OwnerQatariID'] = df_clean['OwnerQatariID'].astype(str).str.strip()
    df_clean['AwardAmount'] = pd.to_numeric(df_clean['AwardAmount'], errors='coerce').round(2)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø¨Ù…Ø¨Ø§Ù„Øº ØºÙŠØ± ØµØ­ÙŠØ­Ø©
    df_clean = df_clean[df_clean['AwardAmount'] > 0].copy()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø±ÙƒØ¨
    df_clean['_CompositeKey'] = (
        df_clean['Season'] + '|' +
        df_clean['Race'] + '|' +
        df_clean['OwnerNumber'] + '|' +
        df_clean['OwnerName'] + '|' +
        df_clean['OwnerQatariID'] + '|' +
        df_clean['AwardAmount'].astype(str)
    )
    
    # Ø¹Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    df_clean['_DuplicateCount'] = df_clean.groupby('_CompositeKey')['_CompositeKey'].transform('count')
    df_clean['_DuplicateGroup'] = df_clean.groupby('_CompositeKey').ngroup()
    
    # ØªØµÙÙŠØ© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙÙ‚Ø· (count >= 2)
    duplicates = df_clean[df_clean['_DuplicateCount'] >= 2].copy()
    
    # ØªØ±ØªÙŠØ¨
    if 'EntryDate' in duplicates.columns:
        duplicates['EntryDate'] = pd.to_datetime(duplicates['EntryDate'], errors='coerce')
        duplicates = duplicates.sort_values(['_DuplicateGroup', 'EntryDate'])
    else:
        duplicates = duplicates.sort_values('_DuplicateGroup')
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
    duplicates['_DuplicateSeverity'] = 'Ù…Ø´ØªØ¨Ù‡'
    duplicates['ReasonText'] = 'âœ… ØªÙƒØ±Ø§Ø± Ù…Ø¤ÙƒØ¯ - Ù…ÙØªØ§Ø­ Ù…Ø±ÙƒØ¨ Ù…ØªØ·Ø§Ø¨Ù‚'
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    total = len(df_clean)
    dup_count = len(duplicates)
    unique_groups = duplicates['_DuplicateGroup'].nunique() if dup_count > 0 else 0
    total_amount = duplicates['AwardAmount'].sum() if dup_count > 0 else 0
    dup_rate = (dup_count / total * 100) if total > 0 else 0
    
    print(f"\n   ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙƒØ´Ù:")
    print(f"      Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©: {total:,}")
    print(f"      Ø³Ø¬Ù„Ø§Øª Ù…ÙƒØ±Ø±Ø©: {dup_count:,}")
    print(f"      Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØªÙƒØ±Ø§Ø±: {unique_groups:,}")
    print(f"      Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±: {dup_rate:.2f}%")
    print(f"      Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¨Ø§Ù„Øº Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {total_amount:,.2f} Ø±ÙŠØ§Ù„")
    
    if unique_groups > 0:
        print(f"\n   ğŸ” Ø£ÙƒØ«Ø± 5 Ø­Ø§Ù„Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹:")
        top = duplicates.groupby('_CompositeKey').size().sort_values(ascending=False).head(5)
        for i, (key, count) in enumerate(top.items(), 1):
            example = duplicates[duplicates['_CompositeKey'] == key].iloc[0]
            print(f"      {i}. {example['OwnerName'][:30]} - {example['Race'][:20]} ({count} Ù…Ø±Ø§Øª)")
    
    return duplicates


def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    print("="*80)
    print("ğŸš€ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† - Enhanced Audit System")
    print("="*80)
    print("ğŸ“‹ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:")
    print("   âœ“ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„")
    print("   âœ“ ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø¯Ù‚Ø© 100%")
    print("   âœ“ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ù†ÙƒÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© (3 ÙØ¦Ø§Øª)")
    print("   âœ“ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† 28 Ø­Ø§Ù„Ø© Ù…Ø¹Ø±ÙˆÙØ©")
    print("   âœ“ 3 ØªÙ‚Ø§Ø±ÙŠØ± Excel Ø§Ø­ØªØ±Ø§ÙÙŠØ©")
    print("="*80)
    
    # Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
    uploads_dir = Path("uploads")
    outputs_dir = Path("outputs")
    outputs_dir.mkdir(exist_ok=True)
    
    # Step 1: Initialize components
    print("\nğŸ”§ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª...")
    normalizer = DataNormalizer()
    bank_matcher = EnhancedBankMatcher(
        ref_last_digits=10,
        amount_tolerance=0.00,
        date_window_days=14
    )
    ground_truth_validator = GroundTruthValidator()
    
    # Step 2: Load awards data
    awards_raw = load_awards_files(uploads_dir)
    
    if len(awards_raw) == 0:
        print("\nâŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬ÙˆØ§Ø¦Ø²")
        return
    
    # Step 3: Normalize awards data
    awards_normalized = normalizer.normalize_dataframe(awards_raw, "Awards")
    
    # Step 4: Detect duplicates
    duplicates = detect_duplicates_enhanced(awards_normalized, normalizer)
    
    if len(duplicates) == 0:
        print("\nâœ… Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªÙƒØ±Ø§Ø±Ø§Øª")
        # ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù„ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± ÙØ§Ø±Øº
    
    # Step 5: Load bank statement
    bank_raw = load_bank_statement(uploads_dir)
    
    if len(bank_raw) > 0:
        # Normalize bank data
        bank_normalized = normalizer.normalize_dataframe(bank_raw, "Bank")
        
        # Step 6: Match with bank
        if len(duplicates) > 0:
            match_results = bank_matcher.match_awards_to_bank(duplicates, bank_normalized)
        else:
            print("\nâš ï¸ ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¨Ù†ÙƒÙŠØ© (Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª)")
    else:
        print("\nâš ï¸ ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¨Ù†ÙƒÙŠØ© (Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙƒØ´Ù Ø¨Ù†Ùƒ)")
    
    # Step 7: Validate ground truth
    if len(duplicates) > 0:
        ground_truth_validator.validate_detection(duplicates)
    else:
        print("\nâš ï¸ ØªØ®Ø·ÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© (Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª)")
    
    # Step 8: Generate comprehensive reports
    report_generator = ComprehensiveReportGenerator(
        duplicates=duplicates,
        normalizer=normalizer,
        bank_matcher=bank_matcher,
        ground_truth_validator=ground_truth_validator,
        output_dir=str(outputs_dir)
    )
    
    generated_files = report_generator.generate_all_reports()
    
    # Final summary
    print("\n" + "="*80)
    print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†")
    print("="*80)
    
    print("\nğŸ“Š Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:")
    
    if len(duplicates) > 0:
        print(f"\nğŸ” Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª:")
        print(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {len(duplicates):,}")
        print(f"   â€¢ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØªÙƒØ±Ø§Ø±: {duplicates['_DuplicateGroup'].nunique():,}")
        print(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¨Ø§Ù„Øº: {duplicates['AwardAmount'].sum():,.2f} Ø±ÙŠØ§Ù„")
    
    if len(bank_matcher.matched_records) > 0 or len(bank_matcher.partial_records) > 0 or len(bank_matcher.unmatched_records) > 0:
        total_bank = len(bank_matcher.matched_records) + len(bank_matcher.partial_records) + len(bank_matcher.unmatched_records)
        matched_pct = (len(bank_matcher.matched_records) / total_bank * 100) if total_bank > 0 else 0
        partial_pct = (len(bank_matcher.partial_records) / total_bank * 100) if total_bank > 0 else 0
        
        print(f"\nğŸ¦ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¨Ù†ÙƒÙŠØ©:")
        print(f"   â€¢ âœ… Ù…Ø·Ø§Ø¨Ù‚ 100%: {len(bank_matcher.matched_records):,} ({matched_pct:.1f}%)")
        print(f"   â€¢ âš ï¸ Ø¬Ø²Ø¦ÙŠ/Ù…Ø´ØªØ¨Ù‡: {len(bank_matcher.partial_records):,} ({partial_pct:.1f}%)")
        print(f"   â€¢ âŒ ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚: {len(bank_matcher.unmatched_records):,} ({100-matched_pct-partial_pct:.1f}%)")
    
    if ground_truth_validator.validation_results:
        results = ground_truth_validator.validation_results
        print(f"\nğŸ¯ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©:")
        print(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª: {results['total_cases']}")
        print(f"   â€¢ Ø­Ø§Ù„Ø§Øª Ù…ÙƒØªØ´ÙØ©: {len(results['detected'])}")
        print(f"   â€¢ Ø­Ø§Ù„Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©: {len(results['missing'])}")
        print(f"   â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù: {results['detection_rate']:.1f}%")
    
    print(f"\nğŸ“ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø© ({len(generated_files)}):")
    for report_type, file_path in generated_files.items():
        print(f"   â€¢ {Path(file_path).name}")
    
    print("\n" + "="*80)
    print("ğŸ‰ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø£Ø®Ø±Ù‰")
    print("="*80)
    
    print("\nğŸ’¡ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø®Ø±Ù‰:")
    print("   1. Ø¶Ø¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ uploads/")
    print("   2. Ø´ØºÙ‘Ù„: python run_enhanced_audit.py")
    print("   3. Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± ÙÙŠ Ù…Ø¬Ù„Ø¯ outputs/")


if __name__ == "__main__":
    main()
